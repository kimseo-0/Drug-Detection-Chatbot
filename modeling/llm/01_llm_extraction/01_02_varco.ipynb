{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615ddf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d95793bd2646e5a8d469e4d993e6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84edc9b621664bf883a071342bfb5250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:2242: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼: ['[ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰]19 ì¤‘', 'Â·ìœ íš¨ì„±ë¶„:', 'ë””í”Œë£¨ì½”ë¥´í†¨ë¡ ë°œë ˆë ˆì´íŠ¸', '(BP) 3mg', 'ì²¨ê°€ì œ(ë³´ì¡´ì œ):', 'íŒŒë¼ì˜¥ì‹œë²¤ì¡°ì‚°ë©”í‹¸(KP)', '1.8mg', 'íŒŒë¼ì˜¥ì‹œë²¤ì¡°ì‚°í”„ë¡œí•„(KP)', '.0.2mg', 'ê¸°íƒ€ì²¨ê°€ì œ:', 'ê²½ì§ˆìœ ë™íŒŒë¼í•€,ë¼ìš°ë¦´í™©ì‚°', 'ë‚˜íŠ¸ë¥¨,ëª¨ë…¸ìŠ¤í…Œì•„ë¥´ì‚°ì†Œë¥´', 'ë¹„íƒ„,ì„¸íƒ„ì˜¬,ìŠ¤í…Œì•„ë¦´ì•Œì½”', 'ì˜¬,ì •ì œìˆ˜,ì¹´ë³´ë¨¸940,íŠ¸', 'ë¡¤ì•„ë¯¼,í”„ë¡œí•„ë Œê¸€ë¦¬ì½œ', '[ì„±ìƒ]', 'í°ìƒ‰~ë¯¸ë‹´í™©ìƒ‰ì˜ ê· ì§ˆí•œ ë¡œì…˜ì œ', '[íš¨ëŠ¥Â·íš¨ê³¼]', 'ì²¨ë¶€ë¬¸ì„œì°¸ì¡°', '[ìš©ë²•Â·ìš©ëŸ‰]', '1ì¼ 2~3íšŒ ì•ê²Œ  ë°”ë¥¸ë‹¤.', 'ì¦ìƒì´ í˜¸ì „ë˜ë©´ 1ì¼ 1íšŒë¡œ', 'ì¶©ë¶„í•˜ë‹¤.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ LLM JSON ê²°ê³¼:\n",
      "{\n",
      "  \"ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰\": [\n",
      "    \"19 ì¤‘\",\n",
      "    \"ìœ íš¨ì„±ë¶„:\",\n",
      "    \"ë””í”Œë£¨ì½”ë¥´í†¨ë¡ ë°œë ˆì´íŠ¸\",\n",
      "    \"(BP) 3mg\",\n",
      "    \"ì²¨ê°€ì œ(ë³´ì¡´ì œ):\",\n",
      "    \"íŒŒë¼ì˜¥ì‹œë²¤ì¡°ì‚°ë©”í‹¸(KP)\",\n",
      "    \"1.8mg\",\n",
      "    \"íŒŒë¼ì˜¥ì‹œë²¤ì¡°ì‚°í”„ë¡œí•„(KP)\",\n",
      "    \".0.2mg\"\n",
      "  ],\n",
      "  \"ì„±ìƒ\": [\n",
      "    \"í°ìƒ‰~ë¯¸ë‹´í™©ìƒ‰ì˜ ê· ì§ˆí•œ ë¡œì…˜ì œ\"\n",
      "  ],\n",
      "  \"íš¨ëŠ¥Â·íš¨ê³¼\": [\n",
      "    \"ì˜¬,ì •ì œìˆ˜,ì¹´ë³´ë¨¸940,íŠ¸\",\n",
      "    \"ë¡¤ì•„ë¯¼,í”„ë¡œí•„ë Œê¸€ë¦¬ì½œ\"\n",
      "  ],\n",
      "  \"ìš©ë²•Â·ìš©ëŸ‰\": [\n",
      "    \"1ì¼ 2~3íšŒ ì•ê²Œ  ë°”ë¥¸ë‹¤.\",\n",
      "    \"ì¦ìƒì´ í˜¸ì „ë˜ë©´ 1ì¼ 1íšŒë¡œ\"\n",
      "  ]\n",
      "}\n",
      "âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: output_medicine_00451.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re  # ì •ê·œí‘œí˜„ì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€\n",
    "from PIL import Image\n",
    "import torch\n",
    "from drugocr import extract_text\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# -----------------------------\n",
    "# 0. ë””ë²„ê¹… ì„¤ì • (ì˜¤ë¥˜ ë°œìƒ ì§€ì  íŒŒì•…ìš©)\n",
    "# -----------------------------\n",
    "# CUDA ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ì •í™•í•œ ìœ„ì¹˜ë¥¼ ì°¾ê¸° ìœ„í•´ ë™ê¸°í™” ëª¨ë“œë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.\n",
    "# ì´ ì¤„ì€ ì½”ë“œì˜ ë§¨ ìœ„ì—, torchë¥¼ importí•˜ê¸° ì „ì— ìœ„ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# -----------------------------\n",
    "# 1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# -----------------------------\n",
    "MODEL_REPO = \"NCSOFT/VARCO-VISION-2.0-1.7B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Using device: {device}\")\n",
    "\n",
    "# AutoProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì €ì™€ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ í•œ ë²ˆì— ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "processor = AutoProcessor.from_pretrained(MODEL_REPO, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_REPO,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # VRAM ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ float16ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OCR + ì´ë¯¸ì§€ â†’ JSON ë³€í™˜ í•¨ìˆ˜\n",
    "# -----------------------------\n",
    "def texts_and_image_to_json(image_path, ocr_texts):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # ìµœì¢… ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "    prompt = (\n",
    "        \"<image>\\n\"\n",
    "        \"ì´ ì´ë¯¸ì§€ë¥¼ ì°¸ê³ í•˜ì—¬ ëª¨ë“  ì•½í’ˆ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”. \"\n",
    "        \"JSON keyëŠ” 'ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰', 'ì„±ìƒ', 'íš¨ëŠ¥Â·íš¨ê³¼', 'ìš©ë²•Â·ìš©ëŸ‰' ë“± ì´ë¯¸ì§€ì— ëª…ì‹œëœ í•­ëª© ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”. \"\n",
    "        \"ê° keyì˜ valueì—ëŠ” í•´ë‹¹ í•­ëª© ì•„ë˜ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í¬í•¨ì‹œí‚¤ì„¸ìš”. \"\n",
    "        \"ì˜¤ì§ JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.\\n\"\n",
    "        f\"OCR í…ìŠ¤íŠ¸: {ocr_texts}\"\n",
    "    )\n",
    "    \n",
    "    # ì…ë ¥ ì¤€ë¹„\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # ëª¨ë¸ ì‹¤í–‰\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    result_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•œ JSON íŒŒì‹± ë¡œì§ ê°œì„ \n",
    "    try:\n",
    "        # ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ì²« ë²ˆì§¸ ìœ íš¨í•œ JSON ê°ì²´ íŒ¨í„´ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "        json_match = re.search(r'\\{[^{}]*\\}', result_text)\n",
    "        \n",
    "        if json_match:\n",
    "            json_string = json_match.group(0)\n",
    "            return json.loads(json_string)\n",
    "        else:\n",
    "            print(f\"âš ï¸ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë¬¸: {result_text}\")\n",
    "            return {\"result_text\": result_text}\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "        return {\"result_text\": result_text}\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"C:\\Potenup\\Drug-Detection-Chatbot\\data\\medicine_00451.jpeg\"\n",
    "    \n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    json_result = texts_and_image_to_json(image_path, ocr_texts)\n",
    "    print(\"ğŸ“Œ LLM JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f21844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from PIL import Image\n",
    "import torch\n",
    "from drugocr import extract_text\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# -----------------------------\n",
    "# 0. ë””ë²„ê¹… ì„¤ì • (ì˜¤ë¥˜ ë°œìƒ ì§€ì  íŒŒì•…ìš©)\n",
    "# -----------------------------\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# -----------------------------\n",
    "# 1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# -----------------------------\n",
    "MODEL_REPO = \"NCSOFT/VARCO-VISION-2.0-1.7B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Using device: {device}\")\n",
    "\n",
    "# ëª¨ë¸ì„ ì§ì ‘ ë¶ˆëŸ¬ì™€ GPUì— ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "processor = AutoProcessor.from_pretrained(MODEL_REPO, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_REPO,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OCR + ì´ë¯¸ì§€ â†’ JSON ë³€í™˜ í•¨ìˆ˜\n",
    "# -----------------------------\n",
    "def texts_and_image_to_json(image_path, ocr_texts):\n",
    "    try:\n",
    "        # ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"âŒ Error: Image file not found at {image_path}\")\n",
    "            return {\"error\": \"Image file not found.\"}\n",
    "            \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading image: {e}\")\n",
    "        return {\"error\": \"Failed to load image.\"}\n",
    "    \n",
    "    # <image> í† í°ì„ í”„ë¡¬í”„íŠ¸ ì‹œì‘ ë¶€ë¶„ì— ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€\n",
    "    prompt = (\n",
    "        \"<image>\"\n",
    "        \"ì•„ë˜ OCR í…ìŠ¤íŠ¸ëŠ” ì°¸ê³ ìš©ì¼ ë¿ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ JSON ê°ì²´ë¥¼ ì™„ì„±í•˜ì„¸ìš”. \"\n",
    "        \"JSON keyëŠ” 'ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰', 'ì„±ìƒ', 'íš¨ëŠ¥Â·íš¨ê³¼', 'ìš©ë²•Â·ìš©ëŸ‰' ë“± ì´ë¯¸ì§€ì— ëª…ì‹œëœ í•­ëª© ì´ë¦„ì…ë‹ˆë‹¤. \"\n",
    "        \"ê° keyì˜ valueì—ëŠ” í•´ë‹¹ í•­ëª© ì•„ë˜ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í¬í•¨ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.\\n\"\n",
    "        \"ì˜¤ì§ JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.\\n\"\n",
    "        f\"OCR í…ìŠ¤íŠ¸: {ocr_texts}\"\n",
    "        \"\\n\\n```json\\n\"\n",
    "    )\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    # ì´ì „ì— inputsì— pixel_valuesê°€ ëˆ„ë½ë˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€\n",
    "    # inputs ë”•ì…”ë„ˆë¦¬ì— 'pixel_values'ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°ë¥¼ ëŒ€ë¹„\n",
    "    if 'pixel_values' not in inputs:\n",
    "        # ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ì²˜ë¦¬í•˜ì—¬ pixel_valuesë¥¼ ì¶”ì¶œ\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        inputs['pixel_values'] = pixel_values\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    result_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    try:\n",
    "        json_matches = re.findall(r'```json\\n([\\s\\S]*?)```', result_text)\n",
    "        if json_matches:\n",
    "            json_string = json_matches[-1]\n",
    "            return json.loads(json_string)\n",
    "\n",
    "        json_matches_fallback = re.findall(r'\\{[\\s\\S]*\\}', result_text)\n",
    "        if json_matches_fallback:\n",
    "            json_string_fallback = json_matches_fallback[-1]\n",
    "            return json.loads(json_string_fallback)\n",
    "        else:\n",
    "            print(f\"âš ï¸ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë¬¸: {result_text}\")\n",
    "            return {\"result_text\": result_text}\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "        return {\"result_text\": result_text}\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00451.jpeg\"\n",
    "    \n",
    "    try:\n",
    "        from drugocr import extract_text\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ 'drugocr' ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„ì‹œ ë”ë¯¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        def extract_text(image_path):\n",
    "            return [\n",
    "                '[ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰]1g ì¤‘',\n",
    "                'Â·ìœ íš¨ì„±ë¶„:',\n",
    "                'ë””í”Œë£¨ì½”ë¥´í†¨ë¡ ë°œë ˆë ˆì´íŠ¸',\n",
    "                '(BP) 3mg',\n",
    "                'ì²¨ê°€ì œ(ë³´ì¡´ì œ):',\n",
    "                'íŒŒë¼ì˜¥ì‹œë²¤ì¡°ì‚°ë©”í‹¸(KP)',\n",
    "                '1.8mg',\n",
    "                'íŒŒë¼ì˜¥ì‹œë²¤ì¡°ì‚°í”„ë¡œí•„(KP)',\n",
    "                '0.2mg',\n",
    "                'ê¸°íƒ€ì²¨ê°€ì œ:',\n",
    "                'ê²½ì§ˆìœ ë™íŒŒë¼í•€, ë¼ìš°ë¦´í™©ì‚°',\n",
    "                'ë‚˜íŠ¸ë¥¨, ëª¨ë…¸ìŠ¤í…Œì•„ë¥´ì‚°ì†Œë¥´',\n",
    "                'ë¹„íƒ„, ì„¸íƒ„ì˜¬, ìŠ¤í…Œì•„ë¦´ì•Œì½”',\n",
    "                'ì˜¬, ì •ì œìˆ˜, ì¹´ë³´ë¨¸940, íŠ¸',\n",
    "                'ë¡¤ì•„ë¯¼, í”„ë¡œí•„ë Œê¸€ë¦¬ì½œ',\n",
    "                '[ì„±ìƒ]',\n",
    "                'í°ìƒ‰~ë¯¸ë‹´í™©ìƒ‰ì˜ ê· ì§ˆí•œ ë¡œì…˜ì œ',\n",
    "                '[íš¨ëŠ¥Â·íš¨ê³¼]',\n",
    "                'ì²¨ë¶€ë¬¸ì„œì°¸ì¡°',\n",
    "                '[ìš©ë²•Â·ìš©ëŸ‰]',\n",
    "                '1ì¼ 2~3íšŒ ì–‡ê²Œ ë°”ë¥¸ë‹¤.',\n",
    "                'ì¦ìƒì´ í˜¸ì „ë˜ë©´ 1ì¼ 1íšŒë¡œ',\n",
    "                'ì¶©ë¶„í•˜ë‹¤.'\n",
    "            ]\n",
    "            \n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    json_result = texts_and_image_to_json(image_path, ocr_texts)\n",
    "    print(\"ğŸ“Œ LLM JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    if 'error' not in json_result:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n",
    "    else:\n",
    "        print(f\"âŒ JSON íŒŒì¼ ì €ì¥ ì‹¤íŒ¨. ì˜¤ë¥˜: {json_result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import json5\n",
    "import re\n",
    "from PIL import Image\n",
    "import torch\n",
    "from drugocr import extract_text\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# -----------------------------\n",
    "# 0. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# -----------------------------\n",
    "MODEL_REPO = \"NCSOFT/VARCO-VISION-2.0-1.7B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Using device: {device}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_REPO, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_REPO,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. í‚¤ íŒ¨í„´ ì •ì˜ (ì¼ë°˜í™”)\n",
    "# -----------------------------\n",
    "KEY_PATTERNS = {\n",
    "    \"ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰\": [r'\\[ì›ë£Œì•½í’ˆ.*\\]'],\n",
    "    \"ìœ íš¨ì„±ë¶„\": [r'Â·?ìœ íš¨ì„±ë¶„:'],\n",
    "    \"ì²¨ê°€ì œ(ë³´ì¡´ì œ)\": [r'ì²¨ê°€ì œ\\(ë³´ì¡´ì œ\\):'],\n",
    "    \"ê¸°íƒ€ì²¨ê°€ì œ\": [r'ê¸°íƒ€ì²¨ê°€ì œ:'],\n",
    "    \"ì„±ìƒ\": [r'\\[ì„±ìƒ\\]'],\n",
    "    \"íš¨ëŠ¥Â·íš¨ê³¼\": [r'\\[íš¨ëŠ¥Â·íš¨ê³¼\\]'],\n",
    "    \"ìš©ë²•Â·ìš©ëŸ‰\": [r'\\[ìš©ë²•Â·ìš©ëŸ‰\\]']\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OCR í›„ì²˜ë¦¬ (ì¼ë°˜í™”)\n",
    "# -----------------------------\n",
    "def preprocess_ocr_general(ocr_texts):\n",
    "    data = {k: None for k in KEY_PATTERNS.keys()}\n",
    "    current_key = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in ocr_texts:\n",
    "        line = line.strip()\n",
    "        # í‚¤ íƒì§€\n",
    "        matched = False\n",
    "        for key, patterns in KEY_PATTERNS.items():\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, line):\n",
    "                    if buffer and current_key:\n",
    "                        data[current_key] = ' '.join(buffer).replace('  ', ' ')\n",
    "                    buffer = []\n",
    "                    current_key = key\n",
    "                    # íŒ¨í„´ ì œê±° í›„ bufferì— ë‚¨ì€ í…ìŠ¤íŠ¸ ì €ì¥\n",
    "                    line_clean = re.sub(pattern, '', line).strip()\n",
    "                    if line_clean:\n",
    "                        buffer.append(line_clean)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        # í‚¤ê°€ ì•„ë‹ˆë©´ í˜„ì¬ bufferì— ì¶”ê°€\n",
    "        if current_key and not matched and line:\n",
    "            buffer.append(line)\n",
    "\n",
    "    # ë§ˆì§€ë§‰ buffer ì²˜ë¦¬\n",
    "    if buffer and current_key:\n",
    "        data[current_key] = ' '.join(buffer).replace('  ', ' ')\n",
    "\n",
    "    return data\n",
    "\n",
    "# -----------------------------\n",
    "# 3. JSON ë¸”ë¡ ì¶”ì¶œ\n",
    "# -----------------------------\n",
    "def extract_json_from_text(text):\n",
    "    text_clean = re.sub(r'```json|```', '', text, flags=re.IGNORECASE).strip()\n",
    "    matches = re.findall(r'\\{.*?\\}', text_clean, flags=re.DOTALL)\n",
    "    if matches:\n",
    "        json_string = matches[-1].replace(\"'\", '\"').strip()\n",
    "        return json_string\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# 4. LLM JSON ë³€í™˜\n",
    "# -----------------------------\n",
    "def texts_and_image_to_json(image_path, ocr_texts):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_ocr = preprocess_ocr_general(ocr_texts)\n",
    "\n",
    "    prompt = (\n",
    "        \"<image>\\n\"\n",
    "        \"ì•„ë˜ OCR í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì•½ ì •ë³´ë§Œ JSONìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”. \"\n",
    "        \"JSON keyëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: \"\n",
    "        + ', '.join(KEY_PATTERNS.keys()) + \". \"\n",
    "        \"ê°’ì´ ì—†ìœ¼ë©´ nullë¡œ í‘œì‹œí•˜ê³ , ê´‘ê³ /ì œì¡°ì‚¬ ë“± ì•½ ì •ë³´ì™€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ë¬´ì‹œí•˜ì„¸ìš”.\\n\"\n",
    "        f\"OCR í•­ëª©ë³„ í…ìŠ¤íŠ¸:\\n{preprocessed_ocr}\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    result_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    json_string = extract_json_from_text(result_text)\n",
    "\n",
    "    if json_string:\n",
    "        try:\n",
    "            data = json5.loads(json_string)\n",
    "            return filter_json_fields(data)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "            return {\"result_text\": result_text}\n",
    "    else:\n",
    "        print(\"âš ï¸ JSON ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return {\"result_text\": result_text}\n",
    "\n",
    "# -----------------------------\n",
    "# 5. í›„ì²˜ë¦¬: í‚¤ í•„í„° + null ì²˜ë¦¬\n",
    "# -----------------------------\n",
    "def filter_json_fields(data):\n",
    "    return {k: data.get(k, None) for k in KEY_PATTERNS.keys()}\n",
    "\n",
    "# -----------------------------\n",
    "# 6. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00806.jpeg\"\n",
    "    \n",
    "    # OCR ì¶”ì¶œ\n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    # LLM JSON ë³€í™˜\n",
    "    json_result = texts_and_image_to_json(image_path, ocr_texts)\n",
    "    print(\"ğŸ“Œ LLM JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1573ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import json5\n",
    "import re\n",
    "from PIL import Image\n",
    "import torch\n",
    "from drugocr import extract_text\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# -----------------------------\n",
    "# 0. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# -----------------------------\n",
    "MODEL_REPO = \"NCSOFT/VARCO-VISION-2.0-1.7B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Using device: {device}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_REPO, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_REPO,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. í‚¤ íŒ¨í„´ ì •ì˜ (ì¼ë°˜í™”)\n",
    "# -----------------------------\n",
    "KEY_PATTERNS = {\n",
    "    \"ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰\": [r'\\[ì›ë£Œì•½í’ˆ.*\\]'],\n",
    "    \"ìœ íš¨ì„±ë¶„\": [r'Â·?ìœ íš¨ì„±ë¶„:'],\n",
    "    \"ì²¨ê°€ì œ(ë³´ì¡´ì œ)\": [r'ì²¨ê°€ì œ\\(ë³´ì¡´ì œ\\):'],\n",
    "    \"ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)\": [r'ì²¨ê°€ì œ\\(ë™ë¬¼ìœ ë˜ì„±ë¶„\\):'],\n",
    "    \"ê¸°íƒ€ì²¨ê°€ì œ\": [r'ê¸°íƒ€ì²¨ê°€ì œ:'],\n",
    "    \"ì„±ìƒ\": [r'\\[ì„±ìƒ\\]'],\n",
    "    \"íš¨ëŠ¥Â·íš¨ê³¼\": [r'\\[íš¨ëŠ¥Â·íš¨ê³¼\\]'],\n",
    "    \"ìš©ë²•Â·ìš©ëŸ‰\": [r'\\[ìš©ë²•Â·ìš©ëŸ‰\\]']\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OCR í›„ì²˜ë¦¬ (ì¼ë°˜í™”)\n",
    "# -----------------------------\n",
    "def preprocess_ocr_general(ocr_texts):\n",
    "    data = {k: None for k in KEY_PATTERNS.keys()}\n",
    "    text_combined = ' '.join(ocr_texts).strip()\n",
    "\n",
    "    # ë„ì–´ì“°ê¸° ë° ì˜¤íƒ€ ë³´ì •\n",
    "    text_combined = re.sub(r'(\\S)(,)(\\S)', r'\\1, \\3', text_combined)\n",
    "    text_combined = re.sub(r'(\\S)(\\()', r'\\1 (', text_combined)\n",
    "    text_combined = re.sub(r'(\\))(\\S)', r') \\2', text_combined)\n",
    "    text_combined = text_combined.replace(\"ì•ê²Œ\", \"ì–‡ê²Œ\")\n",
    "    text_combined = text_combined.replace(\"Â·ìœ íš¨ì„±ë¶„:\", \" ìœ íš¨ì„±ë¶„: \")\n",
    "    text_combined = text_combined.replace(\"â—ì²¨ê°€ì œ\", \" ì²¨ê°€ì œ\")\n",
    "    \n",
    "    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í‚¤ íŒ¨í„´ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©\n",
    "    all_patterns = '|'.join([p for patterns in KEY_PATTERNS.values() for p in patterns])\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ê° í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬\n",
    "    parts = re.split(f'({all_patterns})', text_combined, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    current_key_name = None\n",
    "    for part in parts:\n",
    "        if not part or part.strip() == '':\n",
    "            continue\n",
    "        \n",
    "        # í‚¤ íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸\n",
    "        matched = False\n",
    "        for key_name, patterns in KEY_PATTERNS.items():\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, part, re.IGNORECASE):\n",
    "                    current_key_name = key_name\n",
    "                    # í‚¤ íŒ¨í„´ ì´í›„ì˜ í…ìŠ¤íŠ¸ë¥¼ í˜„ì¬ ê°’ìœ¼ë¡œ ì„¤ì •\n",
    "                    value_text = re.sub(pattern, '', part).strip()\n",
    "                    if value_text:\n",
    "                        if data[current_key_name]:\n",
    "                            data[current_key_name] += ' ' + value_text\n",
    "                        else:\n",
    "                            data[current_key_name] = value_text\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        \n",
    "        # í‚¤ íŒ¨í„´ì— ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ì´ì „ í‚¤ì˜ ê°’ì— ì¶”ê°€\n",
    "        if not matched and current_key_name:\n",
    "            if data[current_key_name]:\n",
    "                data[current_key_name] += ' ' + part.strip()\n",
    "            else:\n",
    "                data[current_key_name] = part.strip()\n",
    "\n",
    "    # 'ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰'ì— ë‹¤ë¥¸ í‚¤ë“¤ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ë¶„ë¦¬\n",
    "    if data['ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰']:\n",
    "        temp_data = data['ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰']\n",
    "        for key, value in data.items():\n",
    "            if key in ['ìœ íš¨ì„±ë¶„', 'ì²¨ê°€ì œ(ë³´ì¡´ì œ)', 'ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)', 'ê¸°íƒ€ì²¨ê°€ì œ'] and value:\n",
    "                temp_data = temp_data.replace(value, '').strip()\n",
    "        data['ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰'] = temp_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. JSON ë¸”ë¡ ì¶”ì¶œ\n",
    "# -----------------------------\n",
    "def extract_json_from_text(text):\n",
    "    text_clean = re.sub(r'```json|```', '', text, flags=re.IGNORECASE).strip()\n",
    "    matches = re.findall(r'\\{.*?\\}', text_clean, flags=re.DOTALL)\n",
    "    if matches:\n",
    "        json_string = matches[-1].replace(\"'\", '\"').strip()\n",
    "        return json_string\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# 4. LLM JSON ë³€í™˜\n",
    "# -----------------------------\n",
    "def texts_and_image_to_json(image_path, ocr_texts):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_ocr = preprocess_ocr_general(ocr_texts)\n",
    "\n",
    "    # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— í¬í•¨\n",
    "    ocr_text_str = json.dumps(preprocessed_ocr, ensure_ascii=False, indent=2)\n",
    "\n",
    "    prompt = (\n",
    "        \"<image>\\n\"\n",
    "        \"ì•„ë˜ OCR í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì•½ ì •ë³´ë§Œ JSONìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”. \"\n",
    "        \"JSON keyëŠ” 'ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰', 'ìœ íš¨ì„±ë¶„', 'ì²¨ê°€ì œ(ë³´ì¡´ì œ)', 'ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)', 'ê¸°íƒ€ì²¨ê°€ì œ', 'ì„±ìƒ', 'íš¨ëŠ¥Â·íš¨ê³¼', 'ìš©ë²•Â·ìš©ëŸ‰'ì…ë‹ˆë‹¤. \"\n",
    "        \"ê°’ì´ ì—†ìœ¼ë©´ nullë¡œ í‘œì‹œí•˜ê³ , ê´‘ê³ /ì œì¡°ì‚¬ ë“± ì•½ ì •ë³´ì™€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ë¬´ì‹œí•˜ì„¸ìš”.\\n\"\n",
    "        f\"OCR í•­ëª©ë³„ í…ìŠ¤íŠ¸:\\n{ocr_text_str}\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    if 'pixel_values' not in inputs:\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        inputs['pixel_values'] = pixel_values\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    result_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    json_string = extract_json_from_text(result_text)\n",
    "\n",
    "    if json_string:\n",
    "        try:\n",
    "            data = json5.loads(json_string)\n",
    "            return filter_json_fields(data)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "            return {\"result_text\": result_text}\n",
    "    else:\n",
    "        print(\"âš ï¸ JSON ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return {\"result_text\": result_text}\n",
    "\n",
    "# -----------------------------\n",
    "# 5. í›„ì²˜ë¦¬: í‚¤ í•„í„° + null ì²˜ë¦¬\n",
    "# -----------------------------\n",
    "def filter_json_fields(data):\n",
    "    return {k: data.get(k, None) for k in KEY_PATTERNS.keys()}\n",
    "\n",
    "# -----------------------------\n",
    "# 6. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00806.jpeg\"\n",
    "    \n",
    "    # OCR ì¶”ì¶œ\n",
    "    try:\n",
    "        from drugocr import extract_text\n",
    "    except ImportError:\n",
    "        def extract_text(image_path):\n",
    "            return [\n",
    "                '[ì›ë£Œì•½í’ˆ ë° ë¶„ëŸ‰] ì´ ì•½ 1ìº¡ìŠ ì¤‘',\n",
    "                'â—ìœ íš¨ì„±ë¶„: ë‘íƒ€ìŠ¤í…Œë¦¬ë“œ(USP)............0.5mg',\n",
    "                'â—ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„): ì ¤ë¼í‹´(ê¸°ì›ë™ë¬¼:ì†Œ, ì‚¬ìš©',\n",
    "                'ë¶€ìœ„: ê°€ì£½)',\n",
    "                'â—ê¸°íƒ€ì²¨ê°€ì œ: ë†ê¸€ë¦¬ì„¸ë¦°, ë¶€í‹¸íˆë“œë¡ì‹œí†¨ë£¨ì—”, ìˆ™',\n",
    "                'ì‹ ì‚°ì ¤ë¼í‹´, í´ë¦¬ì˜¥ì‹¤40ê²½í™”í”¼ë§ˆììœ , í”„ë¡œí•„ë Œê¸€',\n",
    "                'ë¦¬ì½œëª¨ë…¸ë¼ìš°ë ˆì´íŠ¸, í´ë¡ì‚¬ë¨¸124, D-ì†Œë¥´ë¹„í†¨ì•¡',\n",
    "                '[ì„±ìƒ]',\n",
    "                'ë¬´ìƒ‰íˆ¬ëª…í•œ ë‚´ìš©ë¬¼ì´ ë“¤ì–´ìˆëŠ” ë¯¸í™©ìƒ‰ì˜ íˆ¬ëª…í•œ íƒ€ì›í˜• ì—°ì§ˆìº¡ìŠ',\n",
    "                '[íš¨ëŠ¥Â·íš¨ê³¼]',\n",
    "                'ì–‘ì„± ì „ë¦½ì„  ë¹„ëŒ€ì¦ ì¦ìƒì˜ ê°œì„ , ê¸‰ì„± ì„± ìš”ì €ë¥˜ ìœ„í—˜ì„± ê°ì†Œ,',\n",
    "                'ì–‘ì„± ì „ë¦½ì„  ë¹„ëŒ€ì¦ê³¼ ê´€ë ¨ëœ ìˆ˜ìˆ  í•„ìš”ì„± ê°ì†Œ, ì„±ì¸ ë‚¨ì„±(ë§Œ18~50ì„¸)ì˜',\n",
    "                'ë‚¨ì„±í˜• íƒˆëª¨(androgenetic alopecia)ì˜ ì¹˜ë£Œ',\n",
    "                '[ìš©ë²•Â·ìš©ëŸ‰]',\n",
    "                'ì´ ì•½ì˜ ê¶Œì¥ìš©ëŸ‰ì€ 1ì¼ 1íšŒ 1ìº¡ìŠ(0.5mg)ì´ë‹¤. ìº¡ìŠ ë‚´ìš©ë¬¼ì— ë…¸ì¶œì‹œ êµ¬ê°• ì¸ë‘ì ë§‰ì˜ ìê·¹ì„ ì´ˆë˜',\n",
    "                'í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ ì•½ì„ ì”¹ê±°ë‚˜ ìª¼ê°œì§€ ì•Šê³  í†µì§¸ë¡œ ì‚¼ì¼œ ë³µìš©í•´ì•¼ í•œë‹¤. ì´ ì•½ì€ í”¼ë¶€ë¥¼ í†µí•´ì„œ',\n",
    "                'í¡ìˆ˜ëœë‹¤. ë”°ë¼ì„œ ì´ ì•½ì˜ í¡ìˆ˜ ê°€ëŠ¥ì„±ê³¼ ë‚¨ì íƒœì•„ì—ê²Œ ë¯¸ì¹˜ëŠ” íƒœì ê¸°í˜•ì˜ ìœ„í—˜ ê°€ëŠ¥ì„± ë•Œë¬¸ì—',\n",
    "                'ì„ì‹ í–ˆê±°ë‚˜ ì„ì‹ ê¸° ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì—¬ì„±ì´ ì´ ì•½ì„ ì·¨ê¸‰í•´ì„œëŠ” ì•ˆëœë‹¤. ë˜, ì—¬ì„±ì€ ì´ ì•½ì„ ì·¨ê¸‰í• ',\n",
    "                'ë•Œë§ˆë‹¤ ì£¼ì˜í•´ì•¼ í•˜ê³  ëˆ„ì¶œë˜ëŠ” ìº¡ìŠ ê³¼ì˜ ì ‘ì´‰ì„ í”¼í•´ì•¼ í•œë‹¤. ë§Œì•½ ìº¡ìŠì´ ìƒˆì–´ ì´ ì•½ê³¼ ì ‘ì´‰í•œ',\n",
    "                'ê²½ìš°ì—ëŠ” ì ‘ì´‰ë¶€ìœ„ë¥¼ ì¦‰ì‹œ ë¬¼ê³¼ ë¹„ëˆ„ë¡œ ì„¸ì²™í•´ì•¼ í•œë‹¤. ì´í•˜ ì²¨ë¶€ë¬¸ì„œ ì°¸ì¡° [ì €ì¥ë°©ë²•] ë°€íìš©ê¸°,30Â°Cì´í•˜ ë³´ê´€',\n",
    "                '[ì œì¡°ì˜ë¢°ì](ì£¼)ì œë‰´ì›ì‚¬ì´ì–¸ìŠ¤ ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ I|ì „ì˜ë©´ ì‚°ë‹¨ê¸¸ 245 [ì œì¡°ì](ì£¼)ìœ ìœ ì œì•½',\n",
    "                'ì¶©ë¶ ì œì²œì‹œ ë°”ì´ì˜¤ë°¸ë¦¬ 1ë¡œ 94'\n",
    "            ]\n",
    "            \n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    # LLM JSON ë³€í™˜\n",
    "    json_result = texts_and_image_to_json(image_path, ocr_texts)\n",
    "    print(\"ğŸ“Œ LLM JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ed1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import json5\n",
    "import re\n",
    "from PIL import Image\n",
    "import torch\n",
    "from drugocr import extract_text\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# -----------------------------\n",
    "# 0. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# -----------------------------\n",
    "MODEL_REPO = \"NCSOFT/VARCO-VISION-2.0-1.7B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Using device: {device}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_REPO, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_REPO,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. í‚¤ íŒ¨í„´ ì •ì˜ (ë™ë¬¼ìœ ë˜ ì„±ë¶„ í¬í•¨)\n",
    "# -----------------------------\n",
    "KEY_PATTERNS = {\n",
    "    \"ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰\": [r'\\[ì›ë£Œì•½í’ˆ.*\\]'],\n",
    "    \"ìœ íš¨ì„±ë¶„\": [r'Â·?ìœ íš¨ì„±ë¶„:'],\n",
    "    \"ì²¨ê°€ì œ(ë³´ì¡´ì œ)\": [r'ì²¨ê°€ì œ\\(ë³´ì¡´ì œ\\):'],\n",
    "    \"ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)\": [r'ì²¨ê°€ì œ\\(ë™ë¬¼ìœ ë˜ì„±ë¶„\\):'],\n",
    "    \"ê¸°íƒ€ì²¨ê°€ì œ\": [r'ê¸°íƒ€ì²¨ê°€ì œ:'],\n",
    "    \"ì„±ìƒ\": [r'\\[ì„±ìƒ\\]'],\n",
    "    \"íš¨ëŠ¥Â·íš¨ê³¼\": [r'\\[íš¨ëŠ¥Â·íš¨ê³¼\\]'],\n",
    "    \"ìš©ë²•Â·ìš©ëŸ‰\": [r'\\[ìš©ë²•Â·ìš©ëŸ‰\\]']\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OCR í›„ì²˜ë¦¬\n",
    "# -----------------------------\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    # ì¤„ë°”ê¿ˆÂ·ê³µë°± ì •ë¦¬\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # ì œì¡°ì‚¬/ì£¼ì†Œ ì œê±°\n",
    "    text = re.sub(r'\\[ì œì¡°ì˜ë¢°ì\\].*?\\[ì œì¡°ì\\].*', '', text)\n",
    "    # ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­ ì œê±°\n",
    "    text = re.sub(r'\\[ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­\\].*', '', text)\n",
    "    return text.strip() if text.strip() else None\n",
    "\n",
    "def preprocess_ocr_general(ocr_texts):\n",
    "    data = {k: None for k in KEY_PATTERNS.keys()}\n",
    "    current_key = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in ocr_texts:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        matched = False\n",
    "        for key, patterns in KEY_PATTERNS.items():\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, line):\n",
    "                    if buffer and current_key:\n",
    "                        data[current_key] = clean_text(' '.join(buffer))\n",
    "                    buffer = []\n",
    "                    current_key = key\n",
    "                    line_clean = re.sub(pattern, '', line).strip()\n",
    "                    if line_clean:\n",
    "                        buffer.append(line_clean)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        if current_key and not matched:\n",
    "            buffer.append(line)\n",
    "\n",
    "    if buffer and current_key:\n",
    "        data[current_key] = clean_text(' '.join(buffer))\n",
    "\n",
    "    return data\n",
    "\n",
    "# -----------------------------\n",
    "# 3. JSON ì¶”ì¶œ (LLM ì¶œë ¥)\n",
    "# -----------------------------\n",
    "def extract_json_from_text(text):\n",
    "    try:\n",
    "        start = text.find('{')\n",
    "        end = text.rfind('}') + 1\n",
    "        json_text = text[start:end].replace(\"'\", '\"')\n",
    "        return json5.loads(json_text)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# 4. LLM JSON ë³€í™˜\n",
    "# -----------------------------\n",
    "def texts_and_image_to_json(image_path, ocr_texts):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_ocr = preprocess_ocr_general(ocr_texts)\n",
    "\n",
    "    prompt = (\n",
    "        \"<image>\\n\"\n",
    "        \"ì•„ë˜ OCR í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì•½ ì •ë³´ë§Œ JSONìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”. \"\n",
    "        \"JSON keyëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: \"\n",
    "        + ', '.join(KEY_PATTERNS.keys()) + \". \"\n",
    "        \"ê°’ì´ ì—†ìœ¼ë©´ nullë¡œ í‘œì‹œí•˜ê³ , ê´‘ê³ /ì œì¡°ì‚¬/ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­ ë“± ì•½ ì •ë³´ì™€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ë¬´ì‹œí•˜ì„¸ìš”. \"\n",
    "        \"ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ìŒë”°ì˜´í‘œ(\\\") ì‚¬ìš©. ì¶œë ¥ ì™¸ ì„¤ëª… ê¸ˆì§€.\\n\"\n",
    "        f\"OCR í•­ëª©ë³„ í…ìŠ¤íŠ¸:\\n{preprocessed_ocr}\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,  # ì•ˆì •ì  JSON\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    result_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    data = extract_json_from_text(result_text)\n",
    "    if data:\n",
    "        return {k: data.get(k, None) for k in KEY_PATTERNS.keys()}\n",
    "    else:\n",
    "        return {\"result_text\": result_text}\n",
    "\n",
    "# -----------------------------\n",
    "# 5. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00806.jpeg\"\n",
    "    \n",
    "    # OCR ì¶”ì¶œ\n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    # JSON ë³€í™˜\n",
    "    json_result = texts_and_image_to_json(image_path, ocr_texts)\n",
    "    \n",
    "    # JSON ì˜ˆì˜ê²Œ ì¶œë ¥\n",
    "    print(\"ğŸ“Œ ìµœì¢… JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import json5\n",
    "import re\n",
    "from PIL import Image\n",
    "import torch\n",
    "from drugocr import extract_text\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# -----------------------------\n",
    "# 0. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# -----------------------------\n",
    "MODEL_REPO = \"NCSOFT/VARCO-VISION-2.0-1.7B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Using device: {device}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_REPO, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_REPO,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. í‚¤ íŒ¨í„´ ì •ì˜ (ì¼ë°˜í™”)\n",
    "# -----------------------------\n",
    "KEY_PATTERNS = {\n",
    "    \"ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰\": [r'\\[ì›ë£Œì•½í’ˆ.*\\]'],\n",
    "    \"ìœ íš¨ì„±ë¶„\": [r'Â·?ìœ íš¨ì„±ë¶„:'],\n",
    "    \"ì²¨ê°€ì œ(ë³´ì¡´ì œ)\": [r'ì²¨ê°€ì œ\\(ë³´ì¡´ì œ\\):'],\n",
    "    \"ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)\": [r'ì²¨ê°€ì œ\\(ë™ë¬¼ìœ ë˜ì„±ë¶„\\):'],\n",
    "    \"ê¸°íƒ€ì²¨ê°€ì œ\": [r'ê¸°íƒ€ì²¨ê°€ì œ:'],\n",
    "    \"ì„±ìƒ\": [r'\\[ì„±ìƒ\\]'],\n",
    "    \"íš¨ëŠ¥Â·íš¨ê³¼\": [r'\\[íš¨ëŠ¥Â·íš¨ê³¼\\]'],\n",
    "    \"ìš©ë²•Â·ìš©ëŸ‰\": [r'\\[ìš©ë²•Â·ìš©ëŸ‰\\]'],\n",
    "    \"ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­\": [r'\\[ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­\\]']\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OCR í›„ì²˜ë¦¬ (ì¼ë°˜í™”)\n",
    "# -----------------------------\n",
    "def preprocess_ocr_general(ocr_texts):\n",
    "    data = {k: [] for k in KEY_PATTERNS.keys()}\n",
    "    current_key = None\n",
    "\n",
    "    for line in ocr_texts:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        matched = False\n",
    "        for key, patterns in KEY_PATTERNS.items():\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, line):\n",
    "                    # ìƒˆë¡œìš´ í‚¤ ë°œê²¬ ì‹œ ì´ì „ í‚¤ì˜ ë²„í¼ë¥¼ ì €ì¥í•˜ê³  ìƒˆ í‚¤ë¡œ ì „í™˜\n",
    "                    current_key = key\n",
    "                    line_clean = re.sub(pattern, '', line).strip()\n",
    "                    if line_clean:\n",
    "                        data[current_key].append(line_clean)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        \n",
    "        if not matched and current_key:\n",
    "            data[current_key].append(line)\n",
    "\n",
    "    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ í•©ì¹˜ê³  ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "    processed_data = {}\n",
    "    for k, v in data.items():\n",
    "        processed_data[k] = ' '.join(v).replace('  ', ' ') if v else None\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. JSON ë¸”ë¡ ì¶”ì¶œ\n",
    "# -----------------------------\n",
    "def extract_json_from_text(text):\n",
    "    text_clean = re.sub(r'```json|```', '', text, flags=re.IGNORECASE).strip()\n",
    "    matches = re.findall(r'\\{.*?\\}', text_clean, flags=re.DOTALL)\n",
    "    if matches:\n",
    "        json_string = matches[-1].replace(\"'\", '\"').strip()\n",
    "        return json_string\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# 4. LLM JSON ë³€í™˜\n",
    "# -----------------------------\n",
    "def texts_and_image_to_json(image_path, ocr_texts):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_ocr = preprocess_ocr_general(ocr_texts)\n",
    "\n",
    "    ocr_text_str = json.dumps(preprocessed_ocr, ensure_ascii=False, indent=2)\n",
    "\n",
    "    prompt = (\n",
    "        \"<image>\\n\"\n",
    "        \"ì•„ë˜ OCR í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì•½ ì •ë³´ë§Œ JSONìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”. \"\n",
    "        \"JSON keyëŠ” 'ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰', 'ìœ íš¨ì„±ë¶„', 'ì²¨ê°€ì œ(ë³´ì¡´ì œ)', 'ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)', 'ê¸°íƒ€ì²¨ê°€ì œ', 'ì„±ìƒ', 'íš¨ëŠ¥Â·íš¨ê³¼', 'ìš©ë²•Â·ìš©ëŸ‰', 'ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­'ì…ë‹ˆë‹¤. \"\n",
    "        \"ê°’ì´ ì—†ìœ¼ë©´ nullë¡œ í‘œì‹œí•˜ê³ , ê´‘ê³ /ì œì¡°ì‚¬ ë“± ì•½ ì •ë³´ì™€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ë¬´ì‹œí•˜ì„¸ìš”.\\n\"\n",
    "        f\"OCR í•­ëª©ë³„ í…ìŠ¤íŠ¸:\\n{ocr_text_str}\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    if 'pixel_values' not in inputs:\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        inputs['pixel_values'] = pixel_values\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    result_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    json_string = extract_json_from_text(result_text)\n",
    "\n",
    "    if json_string:\n",
    "        try:\n",
    "            data = json5.loads(json_string)\n",
    "            return filter_json_fields(data)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "            return {\"result_text\": result_text}\n",
    "    else:\n",
    "        print(\"âš ï¸ JSON ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return {\"result_text\": result_text}\n",
    "\n",
    "# -----------------------------\n",
    "# 5. í›„ì²˜ë¦¬: í‚¤ í•„í„° + null ì²˜ë¦¬\n",
    "# -----------------------------\n",
    "def filter_json_fields(data):\n",
    "    return {k: data.get(k, None) for k in KEY_PATTERNS.keys()}\n",
    "\n",
    "# -----------------------------\n",
    "# 6. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00685.jpg\"\n",
    "    \n",
    "    # OCR ì¶”ì¶œ\n",
    "    try:\n",
    "        from drugocr import extract_text\n",
    "    except ImportError:\n",
    "        def extract_text(image_path):\n",
    "            return [\n",
    "                '[ì›ë£Œì•½í’ˆ ë° ë¶„ëŸ‰] ì´ ì•½ 1ìº¡ìŠ ì¤‘',\n",
    "                'ìœ íš¨ì„±ë¶„:ë‘íƒ€ìŠ¤í…Œë¦¬ë“œ(USP)0.5mg',\n",
    "                'ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„):ì ¤ë¼í‹´(ê¸°ì›ë™ë¬¼:ì†Œ,ì‚¬ìš©',\n",
    "                'ë¶€ìœ„:ê°€ì£½)',\n",
    "                'ê¸°íƒ€ì²¨ê°€ì œ:ë†ê¸€ë¦¬ì„¸ë¦°,ë¶€í‹¸íˆë“œë¡ì‹œí†¨ë£¨ì—”,ìˆ™',\n",
    "                'ì‹ ì‚°ì ¤ë¼í‹´,í´ë¦¬ì˜¥ì‹¤40ê²½í™”í”¼ë§ˆììœ ,í”„ë¡œí•„ë Œê¸€',\n",
    "                'ë¦¬ì½œëª¨ë…¸ë¼ìš°ë ˆì´íŠ¸,í´ë¡ì‚¬ë¨¸124,D-ì†Œë¥´ë¹„í†¨ì•¡',\n",
    "                '[ì„±ìƒ]',\n",
    "                'ë¬´ìƒ‰íˆ¬ëª…í•œ ë‚´ìš©ë¬¼ì´ ë“¤ì–´ìˆëŠ” ë¯¸í™©ìƒ‰ì˜ íˆ¬ëª…í•œ',\n",
    "                'íƒ€ì›í˜• ì—°ì§ˆìº¡ìŠ',\n",
    "                '[íš¨ëŠ¥Â·íš¨ê³¼]',\n",
    "                'ì–‘ì„± ì „ë¦½ì„  ë¹„ëŒ€ì¦ ì¦ìƒì˜ ê°œì„ ,ê¸‰ì„± ì„± ìš”ì €ë¥˜ ìœ„í—˜ì„±',\n",
    "                'ê°ì†Œ,ì–‘ì„± ì „ë¦½ì„  ë¹„ëŒ€ì¦ê³¼ ê´€ë ¨ëœ ìˆ˜ìˆ  í•„ìš”ì„± ê°ì†Œ,',\n",
    "                'ì„±ì¸ ë‚¨ì„±(ë§Œ18~50ì„¸)ì˜ ë‚¨ì„±í˜• íƒˆëª¨(androgenetic',\n",
    "                'alopecia)ì˜ ì¹˜ë£Œ',\n",
    "                '[ìš©ë²•Â·ìš©ëŸ‰]',\n",
    "                'ì´ ì•½ì˜ ê¶Œì¥ìš©ëŸ‰ì€ 1ì¼ 1íšŒ 1ìº¡ìŠ(0.5mg)ì´ë‹¤.',\n",
    "                'ìº¡ìŠ ë‚´ìš©ë¬¼ì— ë…¸ì¶œì‹œ êµ¬ê°• ì¸ë‘ì ë§‰ì˜ ìê·¹ì„ ì´ˆë˜',\n",
    "                'í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ ì•½ì„ ì‰½ê±°ë‚˜ ìª¼ê°œì§€ ì•Šê³  í†µì§¸ë¡œ',\n",
    "                'ì‚¼ì¼œ ë³µìš©í•´ì•¼ í•œë‹¤. ì´ ì•½ì€ ì‹ì‚¬ì™€ ê´€ê³„ì—†ì´ ë³µìš©í• ',\n",
    "                'ìˆ˜ ìˆë‹¤.ì‹ ì¥ì•  í™˜ì ë˜ëŠ” ë…¸ì¸ í™˜ìì—ì„œ ì´ ì•½ì˜',\n",
    "                'ìš©ëŸ‰ì„ ì¡°ì ˆí•  í•„ìš”ëŠ” ì—†ë‹¤. ê°„ì¥ì•  í™˜ìì—ê²Œ ì´ ì•½ì„',\n",
    "                'íˆ¬ì—¬í•œ ìë£Œê°€ ì—†ê¸° ë•Œë¬¸ì— ê°„ì¥ì•  í™˜ìì—ì„œì˜ ì´',\n",
    "                'ì•½ì˜ ê¶Œì¥ìš©ëŸ‰ì€ í™•ë¦½ë˜ì–´ ìˆì§€ ì•Šë‹¤.',\n",
    "                '[ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­]',\n",
    "                '1.ê²½ê³ 1)ì—¬ì„±ì—ê²Œ ë…¸ì¶œì‹œ ë‚¨ì íƒœì•„ì— ë¯¸ì¹˜ëŠ” ìœ„í—˜ì„± ì´ ì•½ì€',\n",
    "                'í”¼ë¶€ë¥¼ í†µ í†µí•´ì„œ ì„œ í¡ìˆ˜ëœë‹¤.ë”°ë¼ì„œ ì´ ì•½ì˜ í¡ìˆ˜ ê°€ëŠ¥ì„±ê³¼ ë‚¨ì',\n",
    "                'íƒœì•„ì—ê²Œ ë¯¸ì¹˜ëŠ” íƒœì ê¸°í˜•ì˜ ìœ„í—˜ ê°€ëŠ¥ì„± ë•Œë¬¸ì— ì„ì‹ í–ˆê±°ë‚˜',\n",
    "                'ì„ì‹ ê¸° ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì—¬ì„±ì´ ì´ ì•½ì„ ì·¨ê¸‰í•´ì„œëŠ” ì•ˆë“  ëœë‹¤. ë˜,',\n",
    "                'ì—¬ì„±ì€ ì´ ì•½ì„ ì·¨ê¸‰í•  ë•Œë§ˆë‹¤ ì£¼ì˜í•´ì•¼ í•˜ê³  ëˆ„ì¶œë˜ëŠ” ìº¡ìŠ',\n",
    "                'ê³¼ì˜ ì ‘ì´‰ì„ í”¼í•´ì•¼ í•œë‹¤. ë§Œì•½ ìº¡ìŠì´ ìƒˆì–´ ì´ ì•½ê³¼ ì ‘ì´‰í•œ',\n",
    "                'ê²½ìš°ì—ëŠ” ì ‘ì´‰ë¶€ìœ„ë¥¼ ì¦‰ì‹œ ë¬¼ê³¼ ë¹„ëˆ„ë¡œ ì„¸ì²™í•´ì•¼ í•œë‹¤.',\n",
    "                'ì´í•˜ ì²¨ë¶€ë¬¸ì„œ ì°¸ì¡°',\n",
    "                '[ì €ì¥ë°©ë²•] ë°€íìš©ê¸°,30Â°Cì´í•˜ ë³´ê´€',\n",
    "                '[ì œì¡°ì˜ë¢°ì](ì£¼)ì œë‰´ì›ì‚¬ì´ì–¸ìŠ¤',\n",
    "                'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ I|ì „ì˜ë©´ ì‚°ë‹¨ê¸¸ 245',\n",
    "                '[ì œì¡°ì](ì£¼)ìœ ìœ ì œì•½',\n",
    "                'ì¶©ë¶ ì œì²œì‹œ ë°”ì´ì˜¤ë°¸ë¦¬ 1ë¡œ 94'\n",
    "            ]\n",
    "            \n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    # LLM JSON ë³€í™˜\n",
    "    json_result = texts_and_image_to_json(image_path, ocr_texts)\n",
    "    print(\"ğŸ“Œ LLM JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c57156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from PIL import Image\n",
    "from drugocr import extract_text\n",
    "\n",
    "# -----------------------------\n",
    "# 1. í‚¤ íŒ¨í„´ ì •ì˜\n",
    "# -----------------------------\n",
    "KEY_PATTERNS = {\n",
    "    \"ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰\": [r'\\[ì›ë£Œì•½í’ˆ.*\\]'],\n",
    "    \"ìœ íš¨ì„±ë¶„\": [r'ìœ íš¨ì„±ë¶„:'],\n",
    "    \"ì²¨ê°€ì œ(ë³´ì¡´ì œ)\": [r'ì²¨ê°€ì œ\\(ë³´ì¡´ì œ\\):'],\n",
    "    \"ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)\": [r'ì²¨ê°€ì œ\\(ë™ë¬¼ìœ ë˜ì„±ë¶„\\):'],\n",
    "    \"ê¸°íƒ€ì²¨ê°€ì œ\": [r'ê¸°íƒ€ì²¨ê°€ì œ:'],\n",
    "    \"ì„±ìƒ\": [r'\\[ì„±ìƒ\\]'],\n",
    "    \"íš¨ëŠ¥Â·íš¨ê³¼\": [r'\\[íš¨ëŠ¥Â·íš¨ê³¼\\]'],\n",
    "    \"ìš©ë²•Â·ìš©ëŸ‰\": [r'\\[ìš©ë²•Â·ìš©ëŸ‰\\]']\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OCR í›„ì²˜ë¦¬\n",
    "# -----------------------------\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    # ì¤„ë°”ê¿ˆ/ì—¬ëŸ¬ ê³µë°± ì œê±°\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # ê´‘ê³ /ì œì¡°ì‚¬/ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­ ì œê±°\n",
    "    text = re.sub(r'\\[ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­\\].*', '', text)\n",
    "    text = re.sub(r'\\[ì œì¡°ì˜ë¢°ì\\].*?\\[ì œì¡°ì\\].*', '', text)\n",
    "    return text.strip() if text.strip() else None\n",
    "\n",
    "def preprocess_ocr_general(ocr_texts):\n",
    "    data = {k: None for k in KEY_PATTERNS.keys()}\n",
    "    current_key = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in ocr_texts:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        matched = False\n",
    "        for key, patterns in KEY_PATTERNS.items():\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, line):\n",
    "                    if buffer and current_key:\n",
    "                        data[current_key] = clean_text(' '.join(buffer))\n",
    "                    buffer = []\n",
    "                    current_key = key\n",
    "                    line_clean = re.sub(pattern, '', line).strip()\n",
    "                    if line_clean:\n",
    "                        buffer.append(line_clean)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        if current_key and not matched:\n",
    "            buffer.append(line)\n",
    "\n",
    "    if buffer and current_key:\n",
    "        data[current_key] = clean_text(' '.join(buffer))\n",
    "\n",
    "    return data\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00806.jpeg\"\n",
    "    \n",
    "    # OCR ì¶”ì¶œ\n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    # OCR í›„ì²˜ë¦¬ â†’ JSON\n",
    "    json_result = preprocess_ocr_general(ocr_texts)\n",
    "    \n",
    "    # JSON ì˜ˆì˜ê²Œ ì¶œë ¥\n",
    "    print(\"ğŸ“Œ ìµœì¢… JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7acd736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from PIL import Image\n",
    "from drugocr import extract_text\n",
    "\n",
    "# -----------------------------\n",
    "# 1. í‚¤ íŒ¨í„´ ì •ì˜\n",
    "# -----------------------------\n",
    "KEY_PATTERNS = {\n",
    "    \"ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰\": [r'\\[ì›ë£Œì•½í’ˆ.*\\]'],\n",
    "    \"ìœ íš¨ì„±ë¶„\": [r'ìœ íš¨ì„±ë¶„:'],\n",
    "    \"ì²¨ê°€ì œ(ë³´ì¡´ì œ)\": [r'ì²¨ê°€ì œ\\(ë³´ì¡´ì œ\\):'],\n",
    "    \"ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)\": [r'ì²¨ê°€ì œ\\(ë™ë¬¼ìœ ë˜ì„±ë¶„\\):'],\n",
    "    \"ê¸°íƒ€ì²¨ê°€ì œ\": [r'ê¸°íƒ€ì²¨ê°€ì œ:'],\n",
    "    \"ì„±ìƒ\": [r'\\[ì„±ìƒ\\]'],\n",
    "    \"íš¨ëŠ¥Â·íš¨ê³¼\": [r'\\[íš¨ëŠ¥Â·íš¨ê³¼\\]'],\n",
    "    \"ìš©ë²•Â·ìš©ëŸ‰\": [r'\\[ìš©ë²•Â·ìš©ëŸ‰\\]']\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OCR í›„ì²˜ë¦¬\n",
    "# -----------------------------\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    # ì¤„ë°”ê¿ˆ/ì—¬ëŸ¬ ê³µë°± ì œê±°\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # ê´‘ê³ /ì œì¡°ì‚¬/ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­ ì œê±°\n",
    "    text = re.sub(r'\\[ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­\\].*', '', text)\n",
    "    text = re.sub(r'\\[ì œì¡°ì˜ë¢°ì\\].*?\\[ì œì¡°ì\\].*', '', text)\n",
    "    return text.strip() if text.strip() else None\n",
    "\n",
    "def preprocess_ocr_general(ocr_texts):\n",
    "    data = {k: None for k in KEY_PATTERNS.keys()}\n",
    "    current_key = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in ocr_texts:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        matched = False\n",
    "        for key, patterns in KEY_PATTERNS.items():\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, line):\n",
    "                    if buffer and current_key:\n",
    "                        data[current_key] = clean_text(' '.join(buffer))\n",
    "                    buffer = []\n",
    "                    current_key = key\n",
    "                    line_clean = re.sub(pattern, '', line).strip()\n",
    "                    if line_clean:\n",
    "                        buffer.append(line_clean)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        if current_key and not matched:\n",
    "            buffer.append(line)\n",
    "\n",
    "    if buffer and current_key:\n",
    "        data[current_key] = clean_text(' '.join(buffer))\n",
    "\n",
    "    return data\n",
    "\n",
    "# -----------------------------\n",
    "# 3. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00298.jpg\"\n",
    "    \n",
    "    # OCR ì¶”ì¶œ\n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    # OCR í›„ì²˜ë¦¬ â†’ JSON\n",
    "    json_result = preprocess_ocr_general(ocr_texts)\n",
    "    \n",
    "    # JSON ì˜ˆì˜ê²Œ ì¶œë ¥\n",
    "    print(\"ğŸ“Œ ìµœì¢… JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import json5\n",
    "import re\n",
    "from PIL import Image\n",
    "import torch\n",
    "from drugocr import extract_text\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# -----------------------------\n",
    "# 0. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# -----------------------------\n",
    "MODEL_REPO = \"NCSOFT/VARCO-VISION-2.0-1.7B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Using device: {device}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_REPO, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_REPO,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. í‚¤ íŒ¨í„´ ì •ì˜ (ì¼ë°˜í™”)\n",
    "# -----------------------------\n",
    "KEY_PATTERNS = {\n",
    "    \"ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰\": [r'\\[ì›ë£Œì•½í’ˆ.*\\]'],\n",
    "    \"ìœ íš¨ì„±ë¶„\": [r'Â·?ìœ íš¨ì„±ë¶„:'],\n",
    "    \"ì²¨ê°€ì œ(ë³´ì¡´ì œ)\": [r'ì²¨ê°€ì œ\\(ë³´ì¡´ì œ\\):'],\n",
    "    \"ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)\": [r'ì²¨ê°€ì œ\\(ë™ë¬¼ìœ ë˜ì„±ë¶„\\):'],\n",
    "    \"ê¸°íƒ€ì²¨ê°€ì œ\": [r'ê¸°íƒ€ ì²¨ê°€ì œ:'],\n",
    "    \"ì„±ìƒ\": [r'\\[ì„±ìƒ\\]'],\n",
    "    \"íš¨ëŠ¥Â·íš¨ê³¼\": [r'\\[íš¨ëŠ¥.*íš¨ê³¼\\]'],\n",
    "    \"ìš©ë²•Â·ìš©ëŸ‰\": [r'\\[ìš©ë²•.*ìš©ëŸ‰\\]'],\n",
    "    \"ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­\": [r'\\[ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­\\]']\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OCR í›„ì²˜ë¦¬ (ì¼ë°˜í™”)\n",
    "# -----------------------------\n",
    "def preprocess_ocr_general(ocr_texts):\n",
    "    data = {k: None for k in KEY_PATTERNS.keys()}\n",
    "    current_key = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in ocr_texts:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # í‚¤ íŒ¨í„´ì„ ì°¾ì•„ë‚´ê³  ì •ë ¬\n",
    "        found_keys = []\n",
    "        for key, patterns in KEY_PATTERNS.items():\n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, line)\n",
    "                if match:\n",
    "                    found_keys.append((match.start(), key, pattern))\n",
    "        found_keys.sort()\n",
    "        \n",
    "        # í‚¤ íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¦¬\n",
    "        if found_keys:\n",
    "            if current_key and buffer:\n",
    "                data[current_key] = ' '.join(buffer).replace('  ', ' ')\n",
    "            \n",
    "            last_idx = 0\n",
    "            for start_idx, key, pattern in found_keys:\n",
    "                # ì´ì „ í‚¤ì˜ ê°’ ì €ì¥\n",
    "                if last_idx < start_idx and current_key:\n",
    "                    buffer.append(line[last_idx:start_idx].strip())\n",
    "                    \n",
    "                current_key = key\n",
    "                buffer = []\n",
    "                last_idx = re.search(pattern, line).end()\n",
    "                \n",
    "            line_clean = line[last_idx:].strip()\n",
    "            if line_clean:\n",
    "                buffer.append(line_clean)\n",
    "        \n",
    "        elif current_key:\n",
    "            buffer.append(line)\n",
    "    \n",
    "    if current_key and buffer:\n",
    "        data[current_key] = ' '.join(buffer).replace('  ', ' ')\n",
    "\n",
    "    # ëª¨ë“  í‚¤ì— ëŒ€í•´ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "    processed_data = {k: v for k, v in data.items() if v is not None}\n",
    "    \n",
    "    # ì˜ˆì™¸ ì²˜ë¦¬: 'ìœ íš¨ì„±ë¶„'ê³¼ 'ê¸°íƒ€ ì²¨ê°€ì œ'ê°€ í•©ì³ì ¸ ìˆëŠ” ê²½ìš°\n",
    "    if 'ìœ íš¨ì„±ë¶„' in processed_data and 'ê¸°íƒ€ì²¨ê°€ì œ' not in processed_data:\n",
    "        match = re.search(r'(ê¸°íƒ€\\s?ì²¨ê°€ì œ:.*)', processed_data['ìœ íš¨ì„±ë¶„'])\n",
    "        if match:\n",
    "            part_to_move = match.group(1)\n",
    "            processed_data['ê¸°íƒ€ì²¨ê°€ì œ'] = re.sub(r'ê¸°íƒ€\\s?ì²¨ê°€ì œ:', '', part_to_move).strip()\n",
    "            processed_data['ìœ íš¨ì„±ë¶„'] = processed_data['ìœ íš¨ì„±ë¶„'].replace(part_to_move, '').strip()\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. JSON ë¸”ë¡ ì¶”ì¶œ\n",
    "# -----------------------------\n",
    "def extract_json_from_text(text):\n",
    "    text_clean = re.sub(r'```json|```', '', text, flags=re.IGNORECASE).strip()\n",
    "    matches = re.findall(r'\\{.*?\\}', text_clean, flags=re.DOTALL)\n",
    "    if matches:\n",
    "        json_string = matches[-1].replace(\"'\", '\"').strip()\n",
    "        return json_string\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# 4. LLM JSON ë³€í™˜\n",
    "# -----------------------------\n",
    "def texts_and_image_to_json(image_path, ocr_texts):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_ocr = preprocess_ocr_general(ocr_texts)\n",
    "\n",
    "    ocr_text_str = json.dumps(preprocessed_ocr, ensure_ascii=False, indent=2)\n",
    "\n",
    "    prompt = (\n",
    "        \"<image>\\n\"\n",
    "        \"ì•„ë˜ OCR í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì•½ ì •ë³´ë§Œ JSONìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”. \"\n",
    "        \"JSON keyëŠ” 'ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰', 'ìœ íš¨ì„±ë¶„', 'ì²¨ê°€ì œ(ë³´ì¡´ì œ)', 'ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)', 'ê¸°íƒ€ì²¨ê°€ì œ', 'ì„±ìƒ', 'íš¨ëŠ¥Â·íš¨ê³¼', 'ìš©ë²•Â·ìš©ëŸ‰', 'ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­'ì…ë‹ˆë‹¤. \"\n",
    "        \"ê°’ì´ ì—†ìœ¼ë©´ nullë¡œ í‘œì‹œí•˜ê³ , ê´‘ê³ /ì œì¡°ì‚¬ ë“± ì•½ ì •ë³´ì™€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ë¬´ì‹œí•˜ì„¸ìš”.\\n\"\n",
    "        f\"OCR í•­ëª©ë³„ í…ìŠ¤íŠ¸:\\n{ocr_text_str}\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    if 'pixel_values' not in inputs:\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        inputs['pixel_values'] = pixel_values\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    result_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    json_string = extract_json_from_text(result_text)\n",
    "\n",
    "    if json_string:\n",
    "        try:\n",
    "            data = json5.loads(json_string)\n",
    "            return filter_json_fields(data)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "            return {\"result_text\": result_text}\n",
    "    else:\n",
    "        print(\"âš ï¸ JSON ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return {\"result_text\": result_text}\n",
    "\n",
    "# -----------------------------\n",
    "# 5. í›„ì²˜ë¦¬: í‚¤ í•„í„° + null ì²˜ë¦¬\n",
    "# -----------------------------\n",
    "def filter_json_fields(data):\n",
    "    return {k: data.get(k, None) for k in KEY_PATTERNS.keys()}\n",
    "\n",
    "# -----------------------------\n",
    "# 6. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00298.jpg\"\n",
    "    \n",
    "    # OCR ì¶”ì¶œ\n",
    "    try:\n",
    "        from drugocr import extract_text\n",
    "    except ImportError:\n",
    "        def extract_text(image_path):\n",
    "            return ['ì¼€ì´ ìº¡ì •', '[ì›ë£Œì•½í’ˆ ë°ê·¸ë¶„ëŸ‰]ì´ì•½1ì •(206mg) ì¤‘', 'ìœ íš¨ì„±ë¶„:í…Œê³ í”„ë¼ì”(ë³„ê·œ)', 'ê¸°íƒ€ ì²¨ê°€ì œ:D-ë§Œë‹ˆí†¨', 'íˆë“œë¡ì‹œí”„ë¡œí•„ì…€ë£°ë¡œì˜¤ìŠ¤.', 'ì˜¤íŒŒë“œë¼ì´1ë¶„í™ìƒ‰(85F240134)', 'ì¥ë°©í˜€', '[ì„±ìƒ]ì—°í•œ ë¶„í™ìƒ‰ì˜', 'O00', '[íš¨ëŠ¥ íš¨ê³¼]1.ë¯¸ë€ì„±', '2.ë¹„ë¯¸ë€ì„±', '3.ìœ„ê¶¤ì–‘ì˜ ì¹˜ë£Œ', '4.ì†Œí™”ì„± ê¶¤ì–‘', 'ë°•í„°íŒŒì¼ë¡œë¦¬', '[ìš©ë²•Â·ìš©ëŸ‰]ì´ ì•½ì€ ì„±ì¸ì—ê²Œ', '1.ë¯¸ë€ì„±ìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì˜ ì¹˜ë£Œ:1ì¼1íšŒ', 'ì‹ë„ì—¼ì´ ì¹˜ë£Œë˜ì§€ ì•Šê±°ë‚˜', '21 .ë¹„ë¯¸ë€ì„±', 'ìœ„ê¶¤ì–‘ì˜ ì¹˜ë£Œ:1ì¼1íšŒ1íšŒ 50mgì„ 8ì£¼ê°„ ê²½êµ¬íˆ¬ì—¬í•œë‹¤.', 'ìœ„í•œ í•­ìƒì œ', 'ë°›ì•„ì•¼ í•œë‹¤.', '1ì¼2íšŒ 7ì¼ê°„ ê²½êµ¬íˆ¬ì—¬í•œë‹¤', 'ì´ì•½ì€ ì‹ì‚¬ì™€ ê´€ê³„ì—†ì´ íˆ¬ì—¬í•  ìˆ˜ ìˆë‹¤.', '[ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­] ì²¨ë¶€ë¬¸ì„œ ì°¸ì¡°', \"[ì €ì¥ë°©ë²•]ê¸°ë°€ìš©ê¸°,ì‹¤ì˜¨(1~30'C)ë³´ê´€\", 'R', 'K-CAB', 'Tegoprazan 50mg', '50.0mg', 'ë¯¸ê²°ì •ì…€ë£°ë¡œì˜¤ìŠ¤, í¬ë¡œìŠ¤ì¹´ë¥´ë©œë¡œì˜¤ìŠ¤ë‚˜íŠ¸ë¥¨.', 'ì½œë¡œì´ë“œì„±ì´ì‚°í™”ê·œì†Œ. ìŠ¤í…Œì•„ë¥´ì‚°ë§ˆê·¸ë„¤ìŠ˜.', 'í•„ë¦„ì½”íŒ…ì •', 'ìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì˜ ì¹˜ë£Œ', 'ìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì˜ì¹˜ë£Œ', 'ë°/ë˜ëŠ” ë§Œì„± ìœ„ì¶•ì„± ìœ„ì—¼ í™˜ìì—ì„œì˜ í—¬ë¦¬ì½”', 'ì œê· ì„ ìœ„í•œ í•­ìƒì œ ë²¼ìš”ìš”ë²ˆ', 'ì´ëŠ¥í”„ê¸‰', 'ë‹¤ìŒê³¼ ê°™ì´ íˆ¬ì—¬í•œë‹¤', '1íšŒ 50mgì„ 4ì£¼ê°„ ê²½êµ¬íˆ¬ì—¬í•œë‹¤.', 'ì¦ìƒì´ ê³„ì†ë˜ëŠ” í™˜ìì˜ ê²½ìš°4ì£¼ ë”íˆ¬ì—¬í•œë‹¤.', 'ìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì˜ ì¹˜ë£Œ:1ì¼ 1íšŒ,1íšŒ 50mgì„ 4ì£¼ê°„ ê²½êµ¬íˆ¬ì—¬í•œë‹¤.', '4.ì†Œí™”ì„± ê¶¤ì–‘ ë°/ë˜ëŠ” ë§Œì„± ìœ„ì¶•ì„± ìœ„ì—¼ í™˜ìì—ì„œì˜ í—¬ë¦¬ì½”ë°•í„°íŒŒì¼ë¡œë¦¬ ì œê· ì„', 'ë³‘ìš©ìš”ë²•:í—¬ë¦¬ì½”ë°•í„°íŒŒì¼ë¡œë¦¬ ê°ì—¼í™˜ìë“¤ì€ ì œê· ìš”ë²•ìœ¼ë¡œ ì¹˜ë£Œ', 'ì´ì•½50mgê³¼ ì•„ëª©ì‹œì‹¤ë¦°1g.í´ë˜ë¦¬íŠ¸ë¡œë§ˆì´ì‹ 500 mgì„']\n",
    "            \n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    # LLM JSON ë³€í™˜\n",
    "    json_result = texts_and_image_to_json(image_path, ocr_texts)\n",
    "    print(\"ğŸ“Œ LLM JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdc9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import json5\n",
    "import re\n",
    "from PIL import Image\n",
    "import torch\n",
    "from drugocr import extract_text\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# -----------------------------\n",
    "# 0. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# -----------------------------\n",
    "MODEL_REPO = \"NCSOFT/VARCO-VISION-2.0-1.7B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ“Œ Using device: {device}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_REPO, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_REPO,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. OCR í›„ì²˜ë¦¬ (ë²”ìš© ë²„ì „)\n",
    "# -----------------------------\n",
    "def preprocess_ocr_global(ocr_texts):\n",
    "    data = {}\n",
    "    current_key = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in ocr_texts:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # ë™ì ìœ¼ë¡œ í‚¤ íŒ¨í„´ì„ ì°¾ìŒ (ì˜ˆ: [ì›ë£Œì•½í’ˆ ë° ê·¸ ë¶„ëŸ‰] ë˜ëŠ” ìœ íš¨ì„±ë¶„:)\n",
    "        match = re.match(r'\\[(.*?)\\]|\\s*([^:]+):', line)\n",
    "        \n",
    "        if match:\n",
    "            # ì´ì „ í‚¤ì˜ ê°’ ì €ì¥\n",
    "            if current_key and buffer:\n",
    "                data[current_key] = ' '.join(buffer).replace('  ', ' ')\n",
    "            \n",
    "            # ìƒˆ í‚¤ ì¶”ì¶œ ë° ë²„í¼ ì´ˆê¸°í™”\n",
    "            if match.group(1):\n",
    "                current_key = match.group(1).strip()\n",
    "                remaining_text = line[match.end(1) + 1:].strip()\n",
    "            else:\n",
    "                current_key = match.group(2).strip()\n",
    "                remaining_text = line[match.end(2) + 1:].strip()\n",
    "            \n",
    "            buffer = []\n",
    "            if remaining_text:\n",
    "                buffer.append(remaining_text)\n",
    "        \n",
    "        elif current_key:\n",
    "            # í˜„ì¬ í‚¤ì— ëŒ€í•œ ê°’ ì¶”ê°€\n",
    "            buffer.append(line)\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ í‚¤ì˜ ê°’ ì €ì¥\n",
    "    if current_key and buffer:\n",
    "        data[current_key] = ' '.join(buffer).replace('  ', ' ')\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. JSON ë¸”ë¡ ì¶”ì¶œ\n",
    "# -----------------------------\n",
    "def extract_json_from_text(text):\n",
    "    text_clean = re.sub(r'```json|```', '', text, flags=re.IGNORECASE).strip()\n",
    "    matches = re.findall(r'\\{.*?\\}', text_clean, flags=re.DOTALL)\n",
    "    if matches:\n",
    "        json_string = matches[-1].replace(\"'\", '\"').strip()\n",
    "        return json_string\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# 3. LLM JSON ë³€í™˜\n",
    "# -----------------------------\n",
    "def texts_and_image_to_json(image_path, ocr_texts):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_ocr = preprocess_ocr_global(ocr_texts)\n",
    "\n",
    "    ocr_text_str = json.dumps(preprocessed_ocr, ensure_ascii=False, indent=2)\n",
    "\n",
    "    prompt = (\n",
    "        \"<image>\\n\"\n",
    "        \"ì•„ë˜ OCR í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì•½ ì •ë³´ë§Œ JSONìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”. \"\n",
    "        \"ê´‘ê³ /ì œì¡°ì‚¬ ë“± ì•½ ì •ë³´ì™€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ë¬´ì‹œí•˜ê³ , ëª¨ë“  ì •ë³´ë¥¼ JSONì— í¬í•¨í•˜ì„¸ìš”.\\n\"\n",
    "        f\"OCR í•­ëª©ë³„ í…ìŠ¤íŠ¸:\\n{ocr_text_str}\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    if 'pixel_values' not in inputs:\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        inputs['pixel_values'] = pixel_values\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    result_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    json_string = extract_json_from_text(result_text)\n",
    "\n",
    "    if json_string:\n",
    "        try:\n",
    "            data = json5.loads(json_string)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "            return {\"result_text\": result_text}\n",
    "    else:\n",
    "        print(\"âš ï¸ JSON ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return {\"result_text\": result_text}\n",
    "\n",
    "# -----------------------------\n",
    "# 4. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00298.jpg\"\n",
    "    \n",
    "    # OCR ì¶”ì¶œ\n",
    "    try:\n",
    "        from drugocr import extract_text\n",
    "    except ImportError:\n",
    "        def extract_text(image_path):\n",
    "            return ['ì¼€ì´ ìº¡ì •', '[ì›ë£Œì•½í’ˆ ë°ê·¸ë¶„ëŸ‰]ì´ì•½1ì •(206mg) ì¤‘', 'ìœ íš¨ì„±ë¶„:í…Œê³ í”„ë¼ì”(ë³„ê·œ)', 'ê¸°íƒ€ ì²¨ê°€ì œ:D-ë§Œë‹ˆí†¨', 'íˆë“œë¡ì‹œí”„ë¡œí•„ì…€ë£°ë¡œì˜¤ìŠ¤.', 'ì˜¤íŒŒë“œë¼ì´1ë¶„í™ìƒ‰(85F240134)', 'ì¥ë°©í˜€', '[ì„±ìƒ]ì—°í•œ ë¶„í™ìƒ‰ì˜', 'O00', '[íš¨ëŠ¥ íš¨ê³¼]1.ë¯¸ë€ì„±', '2.ë¹„ë¯¸ë€ì„±', '3.ìœ„ê¶¤ì–‘ì˜ ì¹˜ë£Œ', '4.ì†Œí™”ì„± ê¶¤ì–‘', 'ë°•í„°íŒŒì¼ë¡œë¦¬', '[ìš©ë²•Â·ìš©ëŸ‰]ì´ ì•½ì€ ì„±ì¸ì—ê²Œ', '1.ë¯¸ë€ì„±ìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì˜ ì¹˜ë£Œ:1ì¼1íšŒ', 'ì‹ë„ì—¼ì´ ì¹˜ë£Œë˜ì§€ ì•Šê±°ë‚˜', '21 .ë¹„ë¯¸ë€ì„±', 'ìœ„ê¶¤ì–‘ì˜ ì¹˜ë£Œ:1ì¼1íšŒ1íšŒ 50mgì„ 8ì£¼ê°„ ê²½êµ¬íˆ¬ì—¬í•œë‹¤.', 'ìœ„í•œ í•­ìƒì œ', 'ë°›ì•„ì•¼ í•œë‹¤.', '1ì¼2íšŒ 7ì¼ê°„ ê²½êµ¬íˆ¬ì—¬í•œë‹¤', 'ì´ì•½ì€ ì‹ì‚¬ì™€ ê´€ê³„ì—†ì´ íˆ¬ì—¬í•  ìˆ˜ ìˆë‹¤.', '[ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­] ì²¨ë¶€ë¬¸ì„œ ì°¸ì¡°', \"[ì €ì¥ë°©ë²•]ê¸°ë°€ìš©ê¸°,ì‹¤ì˜¨(1~30'C)ë³´ê´€\", 'R', 'K-CAB', 'Tegoprazan 50mg', '50.0mg', 'ë¯¸ê²°ì •ì…€ë£°ë¡œì˜¤ìŠ¤, í¬ë¡œìŠ¤ì¹´ë¥´ë©œë¡œì˜¤ìŠ¤ë‚˜íŠ¸ë¥¨.', 'ì½œë¡œì´ë“œì„±ì´ì‚°í™”ê·œì†Œ. ìŠ¤í…Œì•„ë¥´ì‚°ë§ˆê·¸ë„¤ìŠ˜.', 'í•„ë¦„ì½”íŒ…ì •', 'ìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì˜ ì¹˜ë£Œ', 'ìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì˜ì¹˜ë£Œ', 'ë°/ë˜ëŠ” ë§Œì„± ìœ„ì¶•ì„± ìœ„ì—¼ í™˜ìì—ì„œì˜ í—¬ë¦¬ì½”', 'ì œê· ì„ ìœ„í•œ í•­ìƒì œ ë²¼ìš”ìš”ë²ˆ', 'ì´ëŠ¥í”„ê¸‰', 'ë‹¤ìŒê³¼ ê°™ì´ íˆ¬ì—¬í•œë‹¤', '1íšŒ 50mgì„ 4ì£¼ê°„ ê²½êµ¬íˆ¬ì—¬í•œë‹¤.', 'ì¦ìƒì´ ê³„ì†ë˜ëŠ” í™˜ìì˜ ê²½ìš°4ì£¼ ë”íˆ¬ì—¬í•œë‹¤.', 'ìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì˜ ì¹˜ë£Œ:1ì¼ 1íšŒ,1íšŒ 50mgì„ 4ì£¼ê°„ ê²½êµ¬íˆ¬ì—¬í•œë‹¤.', '4.ì†Œí™”ì„± ê¶¤ì–‘ ë°/ë˜ëŠ” ë§Œì„± ìœ„ì¶•ì„± ìœ„ì—¼ í™˜ìì—ì„œì˜ í—¬ë¦¬ì½”ë°•í„°íŒŒì¼ë¡œë¦¬ ì œê· ì„', 'ë³‘ìš©ìš”ë²•:í—¬ë¦¬ì½”ë°•í„°íŒŒì¼ë¡œë¦¬ ê°ì—¼í™˜ìë“¤ì€ ì œê· ìš”ë²•ìœ¼ë¡œ ì¹˜ë£Œ', 'ì´ì•½50mgê³¼ ì•„ëª©ì‹œì‹¤ë¦°1g.í´ë˜ë¦¬íŠ¸ë¡œë§ˆì´ì‹ 500 mgì„']\n",
    "            \n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    # LLM JSON ë³€í™˜\n",
    "    json_result = texts_and_image_to_json(image_path, ocr_texts)\n",
    "    print(\"ğŸ“Œ LLM JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae98200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from PIL import Image\n",
    "from drugocr import extract_text\n",
    "\n",
    "# -----------------------------\n",
    "# 1. OCR í›„ì²˜ë¦¬\n",
    "# -----------------------------\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    # ì¤„ë°”ê¿ˆ/ì—¬ëŸ¬ ê³µë°± ì œê±°\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # ê´‘ê³ /ì œì¡°ì‚¬/ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­ ì œê±°\n",
    "    text = re.sub(r'\\[ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­\\].*', '', text)\n",
    "    text = re.sub(r'\\[ì œì¡°ì˜ë¢°ì\\].*?\\[ì œì¡°ì\\].*', '', text)\n",
    "    return text.strip() if text.strip() else None\n",
    "\n",
    "def extract_key_value(line):\n",
    "    # ëŒ€ê´„í˜¸ [] í˜•ì‹\n",
    "    match_bracket = re.match(r'\\[(.*?)\\]\\s*(.*)', line)\n",
    "    if match_bracket:\n",
    "        key, value = match_bracket.groups()\n",
    "        return key.strip(), value.strip() if value else None\n",
    "    # ì½œë¡  : í˜•ì‹\n",
    "    match_colon = re.match(r'([^:]+):\\s*(.*)', line)\n",
    "    if match_colon:\n",
    "        key, value = match_colon.groups()\n",
    "        return key.strip(), value.strip() if value else None\n",
    "    return None, line.strip()\n",
    "\n",
    "def preprocess_ocr_global(ocr_texts):\n",
    "    data = {}\n",
    "    current_key = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in ocr_texts:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        key, value = extract_key_value(line)\n",
    "\n",
    "        if key:\n",
    "            # ì´ì „ buffer ì €ì¥\n",
    "            if current_key and buffer:\n",
    "                data[current_key] = clean_text(' '.join(buffer))\n",
    "            current_key = key\n",
    "            buffer = [value] if value else []\n",
    "        else:\n",
    "            if current_key:\n",
    "                buffer.append(value)\n",
    "\n",
    "    if current_key and buffer:\n",
    "        data[current_key] = clean_text(' '.join(buffer))\n",
    "\n",
    "    # value ì—†ëŠ” keyëŠ” null ì²˜ë¦¬\n",
    "    for k in data.keys():\n",
    "        if data[k] is None:\n",
    "            data[k] = None\n",
    "\n",
    "    return data\n",
    "\n",
    "# -----------------------------\n",
    "# 2. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00806.jpeg\"\n",
    "    \n",
    "    # OCR ì¶”ì¶œ\n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    # OCR í›„ì²˜ë¦¬ â†’ JSON\n",
    "    json_result = preprocess_ocr_global(ocr_texts)\n",
    "    \n",
    "    # JSON ì˜ˆì˜ê²Œ ì¶œë ¥\n",
    "    print(\"ğŸ“Œ ìµœì¢… JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe510d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from drugocr import extract_text\n",
    "\n",
    "# -----------------------------\n",
    "# 1. OCR í›„ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# -----------------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"ì¤„ë°”ê¿ˆ, ì—¬ëŸ¬ ê³µë°± ì œê±° ë° ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    # ì¤„ë°”ê¿ˆ, ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ í†µì¼\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # ë¶ˆí•„ìš”í•œ í•­ëª© ì œê±°\n",
    "    text = re.sub(r'\\[ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­\\].*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\[ì œì¡°ì˜ë¢°ì\\].*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\[ì œì¡°ì\\].*', '', text, flags=re.IGNORECASE)\n",
    "    return text.strip() if text.strip() else None\n",
    "\n",
    "def extract_key_value(line):\n",
    "    \"\"\"í•œ ì¤„ì—ì„œ keyì™€ value ì¶”ì¶œ\"\"\"\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None, None\n",
    "\n",
    "    # [key] value í˜•ì‹\n",
    "    match_bracket = re.match(r'\\[(.*?)\\]\\s*(.*)', line)\n",
    "    if match_bracket:\n",
    "        key, value = match_bracket.groups()\n",
    "        return key.strip(), value.strip() if value else None\n",
    "\n",
    "    # key: value í˜•ì‹\n",
    "    match_colon = re.match(r'([^:]+):\\s*(.*)', line)\n",
    "    if match_colon:\n",
    "        key, value = match_colon.groups()\n",
    "        return key.strip(), value.strip() if value else None\n",
    "\n",
    "    return None, line.strip()\n",
    "\n",
    "def preprocess_ocr_global(ocr_texts):\n",
    "    \"\"\"OCR í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ key-value JSONìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    data = {}\n",
    "    current_key = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in ocr_texts:\n",
    "        key, value = extract_key_value(line)\n",
    "\n",
    "        if key:  # ìƒˆë¡œìš´ key ë°œê²¬\n",
    "            # ì´ì „ buffer ì €ì¥\n",
    "            if current_key and buffer:\n",
    "                combined = ' '.join(filter(None, buffer))\n",
    "                cleaned_value = clean_text(combined)\n",
    "                if cleaned_value:\n",
    "                    data[current_key] = cleaned_value\n",
    "            current_key = key\n",
    "            buffer = [value] if value else []\n",
    "        else:\n",
    "            if current_key:\n",
    "                buffer.append(value)\n",
    "\n",
    "    # ë§ˆì§€ë§‰ buffer ì €ì¥\n",
    "    if current_key and buffer:\n",
    "        combined = ' '.join(filter(None, buffer))\n",
    "        cleaned_value = clean_text(combined)\n",
    "        if cleaned_value:\n",
    "            data[current_key] = cleaned_value\n",
    "\n",
    "    return data\n",
    "\n",
    "# -----------------------------\n",
    "# 2. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00806.jpeg\"\n",
    "    \n",
    "    # OCR ì¶”ì¶œ\n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    # JSON ë³€í™˜\n",
    "    json_result = preprocess_ocr_global(ocr_texts)\n",
    "    \n",
    "    # JSON ì˜ˆì˜ê²Œ ì¶œë ¥\n",
    "    print(\"ğŸ“Œ ìµœì¢… JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        # ensure_ascii=False ìœ ì§€, indent=2ë¡œ ì½ê¸° ì‰½ê²Œ ì €ì¥\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec15e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from drugocr import extract_text\n",
    "\n",
    "# -----------------------------\n",
    "# 1. OCR í›„ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# -----------------------------\n",
    "def clean_and_correct_text(text):\n",
    "    \"\"\"ì¤„ë°”ê¿ˆ, ì—¬ëŸ¬ ê³µë°± ì œê±° ë° ë„ì–´ì“°ê¸° êµì •\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    # ì¤„ë°”ê¿ˆ, ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ í†µì¼\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # íŠ¹ì • ë„ì–´ì“°ê¸° ì˜¤ë¥˜ ìˆ˜ë™ êµì • (í•„ìš”í•œ ê²½ìš°)\n",
    "    text = text.replace('ìˆ™ ì‹ ì‚°', 'ìˆ™ì‹ ì‚°')\n",
    "    text = text.replace('í”„ë¡œí•„ë Œê¸€ ë¦¬ì½œ', 'í”„ë¡œí•„ë Œê¸€ë¦¬ì½œ')\n",
    "    text = text.replace('ê¸°íƒ€ì²¨ê°€ì œ:ë†ê¸€ë¦¬ì„¸ë¦°', 'ë†ê¸€ë¦¬ì„¸ë¦°')\n",
    "    \n",
    "    return text.strip() if text.strip() else None\n",
    "\n",
    "def extract_key_value(line):\n",
    "    \"\"\"í•œ ì¤„ì—ì„œ keyì™€ value ì¶”ì¶œ\"\"\"\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None, None\n",
    "\n",
    "    # [key] value í˜•ì‹\n",
    "    match_bracket = re.match(r'\\[(.*?)\\]\\s*(.*)', line)\n",
    "    if match_bracket:\n",
    "        key, value = match_bracket.groups()\n",
    "        return key.strip(), value.strip() if value else None\n",
    "\n",
    "    # key: value í˜•ì‹\n",
    "    match_colon = re.match(r'([^:]+):\\s*(.*)', line)\n",
    "    if match_colon:\n",
    "        key, value = match_colon.groups()\n",
    "        return key.strip(), value.strip() if value else None\n",
    "\n",
    "    return None, line.strip()\n",
    "\n",
    "def preprocess_ocr_global(ocr_texts):\n",
    "    \"\"\"OCR í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ key-value JSONìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    data = {}\n",
    "    current_key = None\n",
    "    buffer = []\n",
    "    \n",
    "    # ì•½ê³¼ ê´€ë ¨ ì—†ëŠ” í•­ëª© í‚¤ ë¦¬ìŠ¤íŠ¸\n",
    "    exclude_keys = ['ì €ì¥ë°©ë²•', 'ì œì¡°ì˜ë¢°ì', 'ì œì¡°ì', 'ì‚¬ìš©ìƒì˜ ì£¼ì˜ì‚¬í•­']\n",
    "\n",
    "    for line in ocr_texts:\n",
    "        key, value = extract_key_value(line)\n",
    "\n",
    "        # 'ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)'ì™€ 'ë¶€ìœ„'ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©\n",
    "        if key == 'ë¶€ìœ„':\n",
    "            if 'ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)' in data:\n",
    "                data['ì²¨ê°€ì œ(ë™ë¬¼ìœ ë˜ì„±ë¶„)'] += ' ' + value\n",
    "            continue # ë‹¤ìŒ ë¼ì¸ìœ¼ë¡œ\n",
    "\n",
    "        if key:  # ìƒˆë¡œìš´ key ë°œê²¬\n",
    "            if current_key and buffer and current_key not in exclude_keys:\n",
    "                combined = ' '.join(filter(None, buffer))\n",
    "                cleaned_value = clean_and_correct_text(combined)\n",
    "                if cleaned_value:\n",
    "                    data[current_key] = cleaned_value\n",
    "            current_key = key\n",
    "            buffer = [value] if value else []\n",
    "        else:\n",
    "            if current_key:\n",
    "                buffer.append(value)\n",
    "\n",
    "    # ë§ˆì§€ë§‰ buffer ì €ì¥\n",
    "    if current_key and buffer and current_key not in exclude_keys:\n",
    "        combined = ' '.join(filter(None, buffer))\n",
    "        cleaned_value = clean_and_correct_text(combined)\n",
    "        if cleaned_value:\n",
    "            data[current_key] = cleaned_value\n",
    "\n",
    "    return data\n",
    "\n",
    "# -----------------------------\n",
    "# 2. ì‹¤í–‰ë¶€\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"/home/wanted-1/PotenupWorkspace/sep-project1/nanhye/__pycache__/data/medicine_00451.jpeg\"\n",
    "    \n",
    "    # OCR ì¶”ì¶œ\n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"ğŸ“Œ OCR ì¶”ì¶œ ê²°ê³¼:\", ocr_texts)\n",
    "    \n",
    "    # JSON ë³€í™˜\n",
    "    json_result = preprocess_ocr_global(ocr_texts)\n",
    "    \n",
    "    # JSON ì˜ˆì˜ê²Œ ì¶œë ¥\n",
    "    print(\"ğŸ“Œ ìµœì¢… JSON ê²°ê³¼:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    # JSON íŒŒì¼ ì €ì¥\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Drug-Detection-Chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
