{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e529fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3682caf701a7420188a88dda1701e566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c36e676bef4f17992ed5e1f18602f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71594d9eb85a4a6aa4c4154d9d449501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for bitsandbytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_REPO, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 4비트 양자화 설정\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m \u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnf4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForVision2Seq\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     28\u001b[0m     MODEL_REPO,\n\u001b[0;32m     29\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mbnb_config,\n\u001b[0;32m     30\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 2. JSON 변환 함수\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\transformers\\utils\\quantization_config.py:510\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.__init__\u001b[1;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    508\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnused kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. These kwargs are not used in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 510\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\transformers\\utils\\quantization_config.py:568\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.post_init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_4bit_use_double_quant, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnb_4bit_use_double_quant must be a boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_in_4bit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbitsandbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m ):\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.10.18-windows-x86_64-none\\lib\\importlib\\metadata\\__init__.py:996\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[0;32m    990\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \n\u001b[0;32m    992\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.10.18-windows-x86_64-none\\lib\\importlib\\metadata\\__init__.py:969\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[0;32m    964\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    965\u001b[0m \n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.10.18-windows-x86_64-none\\lib\\importlib\\metadata\\__init__.py:548\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dist\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes"
     ]
    }
   ],
   "source": [
    "# pip install bitsandbytes -> uv 설치 에러남 \n",
    "# pip install accelerate\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from drugocr import extract_text\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 모델 로드\n",
    "# -----------------------------\n",
    "MODEL_REPO = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"📌 Using device: {device}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_REPO, trust_remote_code=True)\n",
    "\n",
    "# 4비트 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_REPO,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. JSON 변환 함수\n",
    "# -----------------------------\n",
    "def texts_and_image_to_json(image_path, ocr_texts):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    prompt = (\n",
    "        \"아래는 OCR로 추출된 텍스트 목록입니다. \"\n",
    "        \"이 텍스트를 분석해서 key-value 형태의 JSON으로 만들어 주세요.\\n\"\n",
    "        f\"OCR 텍스트: {ocr_texts}\\n\\n\"\n",
    "        \"1. key는 텍스트 의미에 맞게 추론\\n\"\n",
    "        \"2. value는 원문 그대로 유지\\n\"\n",
    "        \"출력 형식은 JSON만 반환\"\n",
    "    )\n",
    "\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs)\n",
    "    result_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    try:\n",
    "        return json.loads(result_text)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"result_text\": result_text}\n",
    "\n",
    "# -----------------------------\n",
    "# 3. main 실행\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"C:\\Potenup\\Drug-Detection-Chatbot\\data\\medicine_00451.jpeg\"\n",
    "\n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"📌 OCR 추출 결과:\", ocr_texts)\n",
    "\n",
    "    json_result = texts_and_image_to_json(image_path, ocr_texts)\n",
    "    print(\"📌 LLM JSON 결과:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ JSON 파일 저장 완료: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0794cb37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bitsandbytes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ PyTorch CUDA 버전:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ GPU 사용 가능 여부:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bitsandbytes'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "print(\"✅ PyTorch CUDA 버전:\", torch.version.cuda)\n",
    "print(\"✅ GPU 사용 가능 여부:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ 현재 GPU 이름:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "try:\n",
    "    from bitsandbytes.nn import Linear4bit\n",
    "    print(\"✅ bitsandbytes 4bit 레이어 불러오기 성공\")\n",
    "\n",
    "    # 간단 테스트 (GPU 올려보기)\n",
    "    lin = Linear4bit(16, 16).cuda()\n",
    "    x = torch.randn(2, 16).cuda()\n",
    "    y = lin(x)\n",
    "    print(\"🚀 4bit 연산 성공, 출력 shape:\", y.shape)\n",
    "except Exception as e:\n",
    "    print(\"❌ bitsandbytes 테스트 실패:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b651e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\paddle\\utils\\cpp_extension\\extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\user\\.paddlex\\official_models\\PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('korean_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\user\\.paddlex\\official_models\\korean_PP-OCRv5_mobile_rec`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083f262d04f64ce79ff7122b83a2008c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5594ca16d1a5407fb4ae57323a3a8c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38156710746c43c49e0f0ce3f6cba73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:2242: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af66d650c25462bbe7b5e9d83d22016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 OCR 추출 결과: ['[원료약품 및 그 분량]19 중', '·유효성분:', '디플루코르톨론발레레이트', '(BP) 3mg', '첨가제(보존제):', '파라옥시벤조산메틸(KP)', '1.8mg', '파라옥시벤조산프로필(KP)', '.0.2mg', '기타첨가제:', '경질유동파라핀,라우릴황산', '나트륨,모노스테아르산소르', '비탄,세탄올,스테아릴알코', '올,정제수,카보머940,트', '롤아민,프로필렌글리콜', '[성상]', '흰색~미담황색의 균질한 로션제', '[효능·효과]', '첨부문서참조', '[용법·용량]', '1일 2~3회 앞게  바른다.', '증상이 호전되면 1일 1회로', '충분하다.']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image features and image tokens do not match: tokens: 0, features 5719",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 OCR 추출 결과:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ocr_texts)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# 이미지+텍스트 → JSON 변환\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m json_result \u001b[38;5;241m=\u001b[39m \u001b[43mtexts_and_image_to_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 LLM JSON 결과:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(json_result, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m, in \u001b[0;36mtexts_and_image_to_json\u001b[1;34m(image_path, ocr_texts)\u001b[0m\n\u001b[0;32m     39\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(\n\u001b[0;32m     40\u001b[0m     text\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m     41\u001b[0m     images\u001b[38;5;241m=\u001b[39m[image],   \u001b[38;5;66;03m# 반드시 리스트 형태\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     43\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 모델 실행\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     47\u001b[0m result_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# JSON 변환 시도\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:2539\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   2529\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2530\u001b[0m         inputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2534\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2535\u001b[0m     )\n\u001b[0;32m   2537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mSAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH):\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2539\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2540\u001b[0m         input_ids,\n\u001b[0;32m   2541\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2542\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2543\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2544\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2545\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2546\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2547\u001b[0m     )\n\u001b[0;32m   2549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2550\u001b[0m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[0;32m   2551\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2552\u001b[0m         input_ids,\n\u001b[0;32m   2553\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2558\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:2867\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2864\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   2866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[1;32m-> 2867\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2868\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:940\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[1;32m--> 940\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    942\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:1369\u001b[0m, in \u001b[0;36mQwen2VLForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m   1364\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_attentions\n\u001b[0;32m   1365\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1366\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1367\u001b[0m )\n\u001b[1;32m-> 1369\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1370\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1371\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[0;32m   1372\u001b[0m     pixel_values_videos\u001b[38;5;241m=\u001b[39mpixel_values_videos,\n\u001b[0;32m   1373\u001b[0m     image_grid_thw\u001b[38;5;241m=\u001b[39mimage_grid_thw,\n\u001b[0;32m   1374\u001b[0m     video_grid_thw\u001b[38;5;241m=\u001b[39mvideo_grid_thw,\n\u001b[0;32m   1375\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1376\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1377\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1378\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1379\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1380\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1381\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1382\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1383\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1384\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1385\u001b[0m )\n\u001b[0;32m   1387\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1388\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:1202\u001b[0m, in \u001b[0;36mQwen2VLModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_image_features(pixel_values, image_grid_thw)\n\u001b[0;32m   1201\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(image_embeds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdevice, inputs_embeds\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 1202\u001b[0m     image_mask, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_placeholder_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeds\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1205\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mmasked_scatter(image_mask, image_embeds)\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values_videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Potenup\\Drug-Detection-Chatbot\\.venv\\lib\\site-packages\\transformers\\models\\qwen2_vl\\modeling_qwen2_vl.py:1148\u001b[0m, in \u001b[0;36mQwen2VLModel.get_placeholder_mask\u001b[1;34m(self, input_ids, inputs_embeds, image_features, video_features)\u001b[0m\n\u001b[0;32m   1146\u001b[0m special_image_mask \u001b[38;5;241m=\u001b[39m special_image_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(inputs_embeds)\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs_embeds[special_image_mask]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnumel():\n\u001b[1;32m-> 1148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage features and image tokens do not match: tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_image_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, features \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1152\u001b[0m n_video_tokens \u001b[38;5;241m=\u001b[39m special_video_mask\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m   1153\u001b[0m special_video_mask \u001b[38;5;241m=\u001b[39m special_video_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(inputs_embeds)\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mValueError\u001b[0m: Image features and image tokens do not match: tokens: 0, features 5719"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from drugocr import extract_text\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 모델 불러오기\n",
    "# -----------------------------\n",
    "MODEL_REPO = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "device = \"cpu\"  # GPU 사용 가능하면 \"cuda\"로 변경\n",
    "print(f\"📌 Using device: {device}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_REPO, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_REPO,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OCR + 이미지 → JSON 변환 함수\n",
    "# -----------------------------\n",
    "def texts_and_image_to_json(image_path, ocr_texts):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Qwen2-VL 입력 프롬프트 (이미지 토큰 <image> 반드시 포함)\n",
    "    prompt = (\n",
    "        \"아래는 OCR로 추출된 텍스트 목록입니다.\\n\"\n",
    "        \"<image>\\n\"  # Qwen2-VL은 반드시 이 토큰 필요\n",
    "        f\"OCR 텍스트: {ocr_texts}\\n\\n\"\n",
    "        \"요구사항:\\n\"\n",
    "        \"1. key는 텍스트 의미에 맞게 추론\\n\"\n",
    "        \"2. value는 원문 그대로 유지\\n\"\n",
    "        \"3. 출력은 JSON만 반환\"\n",
    "    )\n",
    "\n",
    "    # 입력 준비\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=[image],   # 반드시 리스트 형태\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    # 모델 실행\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "    result_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # JSON 변환 시도\n",
    "    try:\n",
    "        return json.loads(result_text)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"result_text\": result_text}\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 실행부\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"C:\\Potenup\\Drug-Detection-Chatbot\\data\\medicine_00451.jpeg\"\n",
    "\n",
    "    # OCR 추출\n",
    "    ocr_texts = extract_text(image_path)\n",
    "    print(\"📌 OCR 추출 결과:\", ocr_texts)\n",
    "\n",
    "    # 이미지+텍스트 → JSON 변환\n",
    "    json_result = texts_and_image_to_json(image_path, ocr_texts)\n",
    "    print(\"📌 LLM JSON 결과:\")\n",
    "    print(json.dumps(json_result, ensure_ascii=False, indent=2))\n",
    "\n",
    "    # JSON 파일 저장\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    output_file = f\"output_{base_name}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ JSON 파일 저장 완료: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Drug-Detection-Chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
