{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d301d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ROBOFLOW_API_KEY = os.environ.get('ROBOFLOW_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66fa4f9",
   "metadata": {},
   "source": [
    "# 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543f13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "model = YOLO(\"./models/yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d3e7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703770e9",
   "metadata": {},
   "source": [
    "# 학습 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42119b2d",
   "metadata": {},
   "source": [
    "### 전처리 1 : 대비 강화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e168b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def enhance_contrast(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(32, 32))\n",
    "    enhanced = clahe.apply(gray)\n",
    "    inverted = cv2.bitwise_not(enhanced)   # 흑백 반전\n",
    "    return cv2.cvtColor(inverted, cv2.COLOR_GRAY2BGR)  # 3채널로 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc507a",
   "metadata": {},
   "source": [
    "## 전처리 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40131ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def enhance_pill_soft(img, clahe_clip=2.0, clahe_tile=8, sharp_amt=0.4, blend=0.6):\n",
    "    # 1) 조명 보정(간단): 큰 블러로 배경 조도 제거 후 보정\n",
    "    blur = cv2.GaussianBlur(img, (0,0), 21)\n",
    "    illum = cv2.addWeighted(img, 1.4, blur, -0.4, 0)  # 부드럽게 대비↑\n",
    "\n",
    "    # 2) LAB에서 L 채널만 CLAHE\n",
    "    lab = cv2.cvtColor(illum, cv2.COLOR_BGR2Lab)\n",
    "    L, A, B = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=clahe_clip, tileGridSize=(clahe_tile, clahe_tile))\n",
    "    L2 = clahe.apply(L)\n",
    "    lab2 = cv2.merge([L2, A, B])\n",
    "    out = cv2.cvtColor(lab2, cv2.COLOR_Lab2BGR)\n",
    "\n",
    "    # 3) 약한 언샤프 마스크(엣지만 살짝)\n",
    "    gauss = cv2.GaussianBlur(out, (0,0), 1.0)\n",
    "    unsharp = cv2.addWeighted(out, 1 + sharp_amt, gauss, -sharp_amt, 0)\n",
    "\n",
    "    # 4) 원본과 소프트 블렌딩(과변형 방지)\n",
    "    final = cv2.addWeighted(unsharp, blend, img, 1.0 - blend, 0)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc88beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_pill_retinex(img, sigma=30, edge_gain=0.15, blend=0.7):\n",
    "    img32 = img.astype(np.float32) + 1.0\n",
    "    base = cv2.GaussianBlur(img32, (0,0), sigma)\n",
    "    ret = cv2.log(img32) - cv2.log(base)          # 간단 MSR 느낌\n",
    "    ret = cv2.normalize(ret, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # 약한 엣지 강화\n",
    "    lap = cv2.Laplacian(ret, cv2.CV_16S, ksize=3)\n",
    "    lap = cv2.convertScaleAbs(lap)\n",
    "    sharpen = cv2.addWeighted(ret, 1.0, lap, edge_gain, 0)\n",
    "\n",
    "    return cv2.addWeighted(sharpen, blend, img, 1.0 - blend, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42b76d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_pill_bilateral(img, d=9, sigmaColor=50, sigmaSpace=50, local_gain=0.8, blend=0.6):\n",
    "    bf = cv2.bilateralFilter(img, d, sigmaColor, sigmaSpace)         # 노이즈 완화 + 엣지 보존\n",
    "    # 로컬 대비(언샤프 변형)\n",
    "    gauss = cv2.GaussianBlur(bf, (0,0), 3.0)\n",
    "    local = cv2.addWeighted(bf, 1 + local_gain, gauss, -local_gain, 0)\n",
    "    return cv2.addWeighted(local, blend, img, 1.0 - blend, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb4238",
   "metadata": {},
   "source": [
    "# 배치 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd69917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from ultralytics.models.yolo.detect import DetectionTrainer\n",
    "\n",
    "class CustomTrainer(DetectionTrainer):\n",
    "    def preprocess_batch(self, batch):\n",
    "        # 부모 클래스의 기본 전처리 로직을 먼저 실행합니다.\n",
    "        # 이렇게 해야 데이터셋 로딩, 리사이징 등의 기본 작업이 정상적으로 수행됩니다.\n",
    "        batch = super().preprocess_batch(batch)\n",
    "\n",
    "        imgs = batch[\"img\"]\n",
    "        dt   = imgs.dtype\n",
    "\n",
    "        # 1. PyTorch 텐서를 NumPy 배열로 변환 (CPU에서 작업)\n",
    "        imgs_np = (imgs.permute(0,2,3,1).cpu().numpy() * 255).astype(np.uint8)\n",
    "        \n",
    "        # 2. 각 이미지에 전처리 적용 (OpenCV 함수 사용)\n",
    "        for i in range(imgs_np.shape[0]):\n",
    "            bgr = imgs_np[i][:, :, ::-1]\n",
    "            bgr = enhance_pill_bilateral(bgr)\n",
    "            imgs_np[i] = bgr[:, :, ::-1]\n",
    "\n",
    "        # 3. NumPy 배열을 다시 PyTorch 텐서로 변환 (CPU에 생성)\n",
    "        processed_imgs = torch.from_numpy(imgs_np).permute(0,3,1,2)\n",
    "\n",
    "        # 4. CPU에서 float 타입으로 변환 후 0-1 스케일링\n",
    "        processed_imgs = processed_imgs.float().div(255)\n",
    "        \n",
    "        # 5. 최종 텐서를 GPU로 이동시키고 원래 데이터 타입으로 변환\n",
    "        batch[\"img\"] = processed_imgs.to(device=device, dtype=dt)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b0607",
   "metadata": {},
   "source": [
    "# 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7209e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ULTRALYTICS_TB\"] = \"1\"  # 텐서보드 로거 활성화\n",
    "\n",
    "PROJECT_NAME = \"pill-detect-1\"\n",
    "DATA_YAML_PATH = f\"./{PROJECT_NAME}/data.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be98c8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 캐시 비우기 성공\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU 메모리 캐시 비우기 성공\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"GPU 메모리 캐시 비우기 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0cd9566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.201  Python-3.10.18 torch-2.8.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4070 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./pill-detect-1/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=540, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=C:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\runs\\pill-detect-1\\train5\\weights\\best.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/pill-detect-1, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\runs\\pill-detect-1\\train, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431062  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,230 parameters, 2,590,214 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 94/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "WARNING imgsz=[540] must be multiple of max stride 32, updating to [544]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1115.2993.3 MB/s, size: 120.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\pill-detect-1\\train\\labels.cache... 3324 images, 3 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 3324/3324 3.3Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 517.2342.1 MB/s, size: 37.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\pill-detect-1\\valid\\labels.cache... 142 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 142/142 140.6Kit/s 0.0s\n",
      "Plotting labels to C:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\runs\\pill-detect-1\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 544 train, 544 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\runs\\pill-detect-1\\train\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/30      2.94G        1.9       1.39      1.549        261        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:08<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.4it/s 1.1s0.3s\n",
      "                   all        142       2714      0.824      0.778      0.854      0.507\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/30      3.29G      1.561      1.029      1.319        435        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:08<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.6it/s 1.1s0.3s\n",
      "                   all        142       2714      0.846      0.812      0.879      0.532\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/30      3.29G      1.504     0.9834       1.29        282        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:13<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.3it/s 1.2s0.4s\n",
      "                   all        142       2714      0.861      0.798      0.882      0.523\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/30      3.66G      1.504     0.9796      1.294        412        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:14<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.0it/s 1.3s0.4s\n",
      "                   all        142       2714      0.795       0.78      0.869      0.517\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/30      3.66G      1.469     0.9429      1.263        599        544: 100% ━━━━━━━━━━━━ 208/208 1.3it/s 2:36<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.1it/s 1.2s0.4s\n",
      "                   all        142       2714      0.849      0.828      0.891      0.535\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/30      3.66G      1.469     0.9275      1.265        298        544: 100% ━━━━━━━━━━━━ 208/208 0.7it/s 5:06<1.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.3it/s 2.2s0.6s\n",
      "                   all        142       2714      0.829      0.856      0.886      0.537\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/30      4.05G      1.447      0.913      1.262        511        544: 100% ━━━━━━━━━━━━ 208/208 1.4it/s 2:31<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.9it/s 1.3s0.4s\n",
      "                   all        142       2714      0.829       0.81      0.871      0.548\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/30      2.97G      1.433     0.8911      1.249        413        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:13<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.6it/s 1.1s0.3s\n",
      "                   all        142       2714      0.895      0.799      0.898      0.557\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/30      2.97G      1.421     0.8776      1.247        670        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:11<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.1it/s 1.2s0.4s\n",
      "                   all        142       2714      0.821      0.852       0.91      0.548\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/30      2.99G      1.417     0.8609      1.238        625        544: 100% ━━━━━━━━━━━━ 208/208 1.5it/s 2:15<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.2it/s 1.2s0.4s\n",
      "                   all        142       2714      0.892      0.836      0.919      0.576\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/30         3G      1.405     0.8517       1.23        495        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:13<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.2it/s 1.2s0.4s\n",
      "                   all        142       2714      0.878      0.828      0.915      0.575\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/30         3G      1.392     0.8398      1.225        335        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:14<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.9it/s 1.3s0.4s\n",
      "                   all        142       2714      0.849      0.833      0.903      0.538\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/30         3G      1.392     0.8287      1.223        463        544: 100% ━━━━━━━━━━━━ 208/208 1.5it/s 2:22<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.1it/s 1.2s0.4s\n",
      "                   all        142       2714      0.877      0.856      0.932      0.575\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/30         3G      1.394     0.8175      1.222        348        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:13<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.9it/s 1.3s0.4s\n",
      "                   all        142       2714      0.935      0.853      0.934      0.574\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/30         3G       1.38     0.8241      1.219        554        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:13<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.4it/s 1.1s0.3s\n",
      "                   all        142       2714      0.884       0.83      0.911      0.573\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/30         3G      1.379     0.8011      1.215        469        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:12<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.0it/s 1.3s0.4s\n",
      "                   all        142       2714        0.9      0.848      0.926      0.582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/30         3G      1.365     0.7945      1.213        237        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:12<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.2it/s 1.2s0.3s\n",
      "                   all        142       2714      0.878      0.859      0.927      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/30         3G      1.363     0.7951      1.214        341        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:13<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.1it/s 1.2s0.4s\n",
      "                   all        142       2714       0.88      0.871       0.93      0.586\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/30      3.47G      1.336     0.7802      1.196        249        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:14<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.3it/s 1.2s0.4s\n",
      "                   all        142       2714      0.908       0.86      0.934      0.588\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/30      3.47G      1.346     0.7749      1.202        492        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:14<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.6it/s 1.1s0.3s\n",
      "                   all        142       2714      0.913       0.85      0.934       0.59\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/30      3.47G      1.309      0.716      1.205        312        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:12<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.4it/s 1.1s0.3s\n",
      "                   all        142       2714      0.916      0.866      0.936       0.59\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/30      3.47G      1.302     0.6975        1.2        238        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:12<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 3.9it/s 1.3s0.4s\n",
      "                   all        142       2714      0.887      0.865      0.933      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/30      3.47G      1.297     0.6947      1.196        303        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:11<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.1it/s 1.2s0.4s\n",
      "                   all        142       2714       0.89      0.881      0.935      0.598\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/30      3.47G      1.288     0.6904      1.195        135        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:13<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.6it/s 1.1s0.3s\n",
      "                   all        142       2714      0.915      0.858      0.936      0.602\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/30      3.47G      1.276     0.6776      1.188        159        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:12<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.5it/s 1.1s0.3s\n",
      "                   all        142       2714       0.89      0.901      0.943      0.601\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/30      3.47G      1.276     0.6706      1.184        161        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:13<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.3it/s 1.2s0.3s\n",
      "                   all        142       2714      0.913      0.876      0.933        0.6\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/30      3.47G      1.267     0.6617      1.183        328        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:13<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.2it/s 1.2s0.4s\n",
      "                   all        142       2714        0.9      0.893      0.945      0.604\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/30      3.47G      1.257     0.6619      1.182        353        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:10<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.2it/s 1.2s0.3s\n",
      "                   all        142       2714      0.908      0.869      0.934      0.597\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/30      3.47G      1.253     0.6509      1.175        251        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:12<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.5it/s 1.1s0.3s\n",
      "                   all        142       2714       0.92      0.871      0.942      0.611\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/30      3.47G      1.248     0.6483      1.174        139        544: 100% ━━━━━━━━━━━━ 208/208 1.6it/s 2:10<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 4.0it/s 1.2s0.4s\n",
      "                   all        142       2714      0.906      0.892      0.944      0.613\n",
      "\n",
      "30 epochs completed in 1.184 hours.\n",
      "Optimizer stripped from C:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\runs\\pill-detect-1\\train\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from C:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\runs\\pill-detect-1\\train\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating C:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\runs\\pill-detect-1\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.201  Python-3.10.18 torch-2.8.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4070 Laptop GPU, 8188MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 5/5 2.1it/s 2.4s0.7s\n",
      "                   all        142       2714      0.907      0.893      0.944      0.613\n",
      "              capsules         73        805      0.898      0.835      0.919      0.584\n",
      "               tablets         87       1909      0.916      0.951      0.969      0.641\n",
      "Speed: 0.4ms preprocess, 3.5ms inference, 0.0ms loss, 4.2ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\runs\\pill-detect-1\\train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "# 학습\n",
    "results = model.train(\n",
    "    data=DATA_YAML_PATH,\n",
    "    epochs=30,\n",
    "    batch=16,\n",
    "    imgsz=540,\n",
    "    device=device,\n",
    "    project=f\"runs/{PROJECT_NAME}\",\n",
    "    save=True,\n",
    "    save_period=10,\n",
    "    verbose=True,\n",
    "    trainer = CustomTrainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86ba92",
   "metadata": {},
   "source": [
    "# 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9e7d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"C:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\\\runs\\pill-detect-1\\\\train\\weights\\\\best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5046b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = YOLO(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "231b832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.201  Python-3.10.18 torch-2.8.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4070 Laptop GPU, 8188MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 94.571.9 MB/s, size: 58.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\pill-detect-1\\valid\\labels.cache... 142 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 142/142  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 2.5it/s 3.6s0.2ss\n",
      "                   all        142       2714      0.927       0.89      0.951      0.615\n",
      "              capsules         73        805      0.914      0.843       0.93      0.588\n",
      "               tablets         87       1909       0.94      0.937      0.972      0.642\n",
      "Speed: 0.8ms preprocess, 16.1ms inference, 0.0ms loss, 2.3ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\runs\\pill-detect-1\\val2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "val_results = load_model.val(\n",
    "    data=DATA_YAML_PATH, \n",
    "    imgsz=640, \n",
    "    iou=0.5, \n",
    "    save=True,\n",
    "    project=f\"runs/{PROJECT_NAME}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05be5dc",
   "metadata": {},
   "source": [
    "# 테스트 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27fa0f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/3 c:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\images\\preprocessed\\test1.jpg: 544x320 4 tabletss, 45.9ms\n",
      "image 2/3 c:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\images\\preprocessed\\test2.jpg: 544x320 4 tabletss, 39.9ms\n",
      "image 3/3 c:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\images\\preprocessed\\test3.jpg: 544x544 1 capsules, 48.4ms\n",
      "Speed: 2.6ms preprocess, 44.8ms inference, 5.2ms postprocess per image at shape (1, 3, 544, 544)\n",
      "Results saved to \u001b[1mC:\\Potenup\\Drug-Detection-Chatbot\\modeling\\segment\\images\\results\\yolo\\predict19\u001b[0m\n",
      "결과 이미지가 ./images/results/yolo 폴더에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "input_dir = \"./images/original\"\n",
    "preprocessed_dir = \"./images/preprocessed\"\n",
    "output_dir = \"./images/results/yolo\"\n",
    "\n",
    "os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for fname in os.listdir(input_dir):\n",
    "    if fname.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        img_path = os.path.join(input_dir, fname)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        img = enhance_pill_bilateral(img)\n",
    "\n",
    "        # 저장\n",
    "        out_path = os.path.join(preprocessed_dir, fname)\n",
    "        cv2.imwrite(out_path, img)\n",
    "\n",
    "model = YOLO(best_model_path)\n",
    "results = model.predict(\n",
    "    source=preprocessed_dir,\n",
    "    conf=0.5,\n",
    "    save=True,\n",
    "    project=output_dir,\n",
    "    name=\"\",\n",
    ")\n",
    "\n",
    "print(f\"결과 이미지가 {output_dir} 폴더에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drug-detection-chatbot (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
